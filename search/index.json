[{"content":"This is a note  本文同时是 系统与计算神经科学22-23秋季 余山老师课程第三次作业.   文献信息  标题 : Signal propagation via cortical hierarchies 期刊 : network neuroscience 作者 : Bertha Vázquez-Rodríguez 、Zhen-Qi Liu DOI : 10.1162/netn_a_00153 Keywords : Brain connectivity; Connectome; Navigation; Neural communication; Neural networks.  术语定义   Shortest path 拓扑最短路径 ： 两个节点间的最小连续边集\n  association areas 关联区域 : 关联区域整合传入的感觉信息，并在感觉和运动区域之间形成连接。\n  Unimodal association cortices 单模态关联皮层 ：同义词 Secondary association cortex，也为Unimodal Cortex。 主要处理来自一种感觉方式的信息的关联区域，已定义两种（感觉和运动），单模态感觉关联皮层有三种典型（视、听、躯体感觉）, 单模态皮层被认为是负责解码(分析)和/或重新编码(合成)从各自的初级皮层接收到的感觉输入。\n  Heteromodal association cortex 多模态关联皮层 ：同义词 Tertiary（第三级）cortex 和 Heteromodal Cortex。 指接收来自多个感觉或多模式区域的输入的区域。由于它们的多模式输入，这些皮层区域被认为是负责更复杂或综合的认知活动。\n  Unimodal-transmodal hierarchy : 将单模态感觉运动皮层与跨模态关联皮层分离的连续轴。 (transmodal 我认为等价于 heteromodal)\n  intrinsic networks 固有（内在）网络 : 大脑区域的子网络，具有连贯的时间过程，通过聚类、独立成分分析或社区检测识别出来。   这篇文章中，简写 ：DM = default mode, FP = frontoparietal, LIM = limbic, VA = ventral attention, DA = dorsal attention, SM = somatomotor, VIS = visual.\n  研究简述 信号的传播很可能受到皮层分层环路组织的限制，这种连续的轴或梯度可以在皮层的功能结构中观察到。\n研究了功能层次如何塑造信号的传播。研究者重建结构网络（目前的重建仅包括皮质区域，忽略了皮质下和小脑）上的路径（拓扑最短路径是推断网络中潜在信号路径的一种简单方法），并通过 Unimodal-transmodal hierarchy 梯度追踪它们的轨迹。发现大脑皮层的分层组织限制了路径轨迹，因此许多路径遵循规范的自下而上(向上升级)或自上而下(向下降级)的轨迹。\n研究结果 path motifs   结构和功能网络分别由dwMRI和静息态fMRI重建,在两个节点之间的最短路径上遇到的节点注释序列。  path motifs  首先研究路径如何映射到假定的Unimodal-transmodal hierarchy，层次被划分为10个等级，上图展示了源节点的三个级别(2、6和9; 行)和目标节点的三个级别(列)的 path motif，阴影区域表示95%的置信区间，颜色用于划分路径长度。\n对于大多数路径而言，motifs 通过层级顺序过渡，但在过渡的性质上存在着系统性的差异。穿越层次结构中较大差异的路径(如2到9)往往遵循更为单调的轨迹。当源节点和目标节点在层次结构中占据相同或相邻的位置时，路径更可能遵循 U 形，有效地绕道到层次结构的中间部分(如2到2)。文章的支持材料里放了与具有随机拓扑和随机层次标签的 Null 网络的轨迹，得到结论是 path motifs既受网络拓扑结构的影响，也受 Unimodal-transmodal hierarchy 的空间结构的影响。\n拐点  通讯流中的拐点 \n (A) 一个路径序列的示意，红点表示这个点在层次结构上是上升的(正斜率，从单模态皮层到多模态皮层)，蓝点相反，更大的点表示转折点。 (B) 每个节点的平均斜率(y)与其在层次结构中的位置成反相关。主要表现为正平均斜率的区域是辅助运动区、躯体运动皮层和视觉皮层。具有负斜率的区域包括前额叶皮层、后顶叶皮层、听觉皮层和颞下皮层。 (C) 七个固有网络的平均斜率，边缘和默认网络平均斜率是负的 (D) 上下转折点的平均概率。背侧注意和腹侧注意出现上转折点出现概率最大，默认网络出现下转折点的概率最大。 (E-F) 单个区域出现转折概率，上转折点最可能出现在注意力相关的网络中，下转折点最可能出现在上额叶和背外侧前额叶，颞下叶和颞叶外侧皮质。  基于层次的导航  Navigation ：一种去中心化的通信机制，信号被转发到距离目标最近的连接邻居。  想知道是否可以通过遵循层次结构来重新概括网络的路径体系结构。所以研究了如果信号被转发到Unimodal-transmodal hierarchy 结构中最接近目标节点的邻居，那么信号是否可以重现网络的路径结构。为了将导航量化为一个通信过程，研究者测量成功恢复的路径比例 $S_R$。\n这里重定义了距离，线性组合由参数β加权的三维欧氏距离和的层次距离，两个值都被归一化到 $[0,1]$ ,导航可以简单地反映成本最小化策略,将 β 从0调整到1，并找到使导航成功率最大化的 β。\n$$d(i,j)=\\beta\\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2} + (1-\\beta)(h_i-h_j)$$  网络拓扑可以在多大程度上重述最短路径。 \n B 显示了使每个源-目标对的导航成功率最大化的β参数的分布,在两个极值处有显著的峰值，表明大多数节点对层次导航或空间导航都有强偏好。 C ：通过对固有网络包含的节点按彼此间的关系进行分层，发现每个网络都有一个独特的指纹。视觉系统、外侧颞皮质和背外侧前额叶皮质的部分区域对空间导航表现出强烈的偏好（红色；β\u0026gt;0.8）；内侧顶叶皮质、内侧前额叶皮层和左侧颞顶叶皮质表现出对分层导航的强烈偏好（蓝色；β\u0026lt;0.2）。  研究方法 数据采集  66健康青年（16女，25.3正负4.9岁），无任务要求  对白质灰质对比敏感的 magnetization-prepared rapid acquisition gradient echo (MPRAGE) sequence DSI（扩散光谱成像）序列 对BOLD对比敏感的梯度回波EPI序列    结构网络重建 灰质被分割成68个皮层节点(Desikan et al., 2006)，这些区域进一步被分为1000个左右大小相近的节点(Cammoun et al., 2012)，参与者的结构连接性使用确定性流线型纤维束成像评估（deterministic streamline tractography），程序在 Connectome Mapper Toolkit (Daducci et al., 2012)。区域对之间的结构连通性被定义为由两个区域的平均流线长度和平均表面积归一化的流线数目，称为纤维密度(Hagmann et al., 2008)。为了减轻对个体参与者连接重建不一致，采用了一种叫群体共识方法（group-consensus approach）。\n功能网络重建 矫正FMRI数据后，对BOLD时间序列进行低通滤波，去掉前四个时间点，数据根据定义结构网络的相同atlas分组后，单个个体的功能连接矩阵被定义为FMRI BOLD时间序列的0时延皮尔逊相关性，群体在这基础上做平均。\nDiffusion Map Embedding 扩散映射嵌入是一种非线性降维算法(Coifman et al.,2005) ,使用的工具箱 Dimensionality Reduction Toolbox.\n最短路径检索 定义了一个拓扑距离度量，将加权邻接矩阵通过 $L =-log(W)$ 变换为连接长度矩阵，把更大权重的连接映射为更短的边。使用 Floyd-Warshall 算法恢复加权最短路径，会在所有源-目标对之间产生唯一的路径。\n斜率与拐点 节点都有一个相关的层次位置值 $h(v_i)$ ,因此节点 $v_i$ 在某一条路径上的斜率为 $s_{v_i} = h(v_{i+1}) - h(v_i)$ ,平均斜率就是这个节点参与的所有路径的均值。\n转移概率  转移概率：信号从源节点转发到目标节点的概率。 $\\sharp{sp:\\circ}$ : 表示满足条件 $\\circ$ 的所有最短路径集合中最短路径数量  对于 1-hop 转移概率矩阵 $T$ ，$i,j$ 表示层次结构、$t$ 是位置\n$$T_{ij}(t) = \\frac{\\sharp{sp:sp(t) = i,sp(t+1)=j}}{\\sharp{sp:length(sp)\\geq t+1}} $$\n个人想法   对文章提到的 Unimodal-transmodal hierarchy 定义的脑区的层次，我个人的理解是层次越高，涉及到（接受）的信息的模态越多，对信息的复杂整合也越多。\n  我认为比较有意思的，在单模态到单模态节点的路径motifs 和 多模态到多模态节点的路径 motifs，除非很短的路径，都会探到中等层次的皮层，路径越长越明显；而中等层次到中等层次motifs的曲线是接近水平的。是否意味着Unimodal-transmodal hierarchy 中的中间层次会更多的负责区域间信息交流/或是连接更广泛。\n  在拐点那张图中，有一个结果：背侧注意和腹侧注意出现上转折点出现概率最大（特别是背侧注意网络），默认网络出现下转折点的概率最大 。考虑到这里提到的网络具有如下特点：\n 背侧注意网络的功能是提供自上而下的注意定向（根据期望和有意识的目标，有选择性地挑选需集中注意力的地方），腹侧注意网络负责自下而上的注意定向 (对前意识刺激（如运动、意外的噪声或视觉变换）的注意力转移) 。 通常在个体清醒静息的状态，不专注于外界时默认模式网络就会活动。  那么是否说明默认网络最有可能是 Unimodal-transmodal hierarchy 层次较低区域之间的信息中转，背侧注意和腹侧注意网络最有可能作为较高层次区域之间的信息中转。 注意机制（不论是自上而下和自下而上的注意定向）作用于经过注意网络传递到的多模态脑区。\n  原文中提到下面这一句，之前并未了解过，我也对提到的理论产生一定兴趣，有时间会阅读相关的文献。 ?  The finding that attention networks are anatomically positioned to redirect signal traffic resonates with modern theories of how attention and control networks shape fluid transitions between segregated and integrated states, guiding adaptive reconfiguration during rest and task (de Pasquale et al., 2012; Fair et al., 2007; Miši ́ c, Fatima et al., 2014; Mohr et al., 2016; Shine et al., 2016).  \n额外参考  Unimodal Cortex association areas Heteromodal association cortex intrinsic networks 几种常见的功能性脑网络划分方式 脑网络组基础 大脑皮层中各层的分工  ","date":"2022-10-11T14:20:38+08:00","image":"https://pic.jitudisk.com/public/2022/10/13/41255e9640096.png","permalink":"https://deathsprout.github.io/p/signal-propagation-via-cortical-hierarchies/","title":"Signal propagation via cortical hierarchies"},{"content":"This is a note  本文同时是 系统与计算神经科学22-23秋季 余山老师课程第二次作业. ||   研究意义 内侧颞叶（MTL）被认为支持情景记忆，即在特定时间、特定地点对特定事件的生动回忆。一些研究表明海马和 MTL 中的神经元也具有很强的时间相关性。 文章搭建了一个简单的数学框架，将空间位置和时间的函数作为更一般计算的特殊情况进行计算。描述的方程做出了一些定性和定量预测，可以用现有技术进行测试。并且也有一些实验结果符合它的预测。\n这个用来表示时间的方程，也适用于表示空间，数感，和基于信息收集的决策\n方法 数学框架：  $x$ : 外部世界的物理量 （如视网膜的位置或声音频率） $f(x)$ ： 这个物理量的函数 $x^*$ : 一个映射，从物理变量映射到其激发细胞发放相关的顺序。一个隐变量？ $\\widetilde{f}(x^*)$ : $f(x)$的内部表示形式   \n 如何近似时间函数 $f(\\Tau)$的示意图  目标是能构建到现在 $\\tau$ 为止的所有时间点的函数 $f(\\tau' \u0026lt; \\tau)$，每一个时间点，现在的值都驱动一个中间表示 $F(s)$， 中间表示的细胞是速率常数 $s$ 的 leaky integrators，时间常数仅 $1/s$。用一个线性算子 $L^{-1}_k$ 构造从 $F(s)$ 到 $f(\\tau)$ 的估计，这个估计是 $\\widetilde{f}(x^*)$ ，是 $f(\\tau)$ 的一个不精确估计，估计的时间精度时间越久越低。\n 通过一个方程 $\\alpha(\\tau)$ 调节这个漏积分的框架，如果 $\\alpha(\\tau)$ 是某物理量 $x$ 的变化速率，以及 $f(\\tau)$ 只是 $x(\\tau)$ 的方程，那么 $\\widetilde{f}$ 是一个估计 $x$ 的方程而不是时间的，写作 $\\widetilde{f}(x^*)$。\n leaky integrators  漏积分方程是一个特定的微分方程，用于描述对输入进行积分但随着时间的推移逐渐泄漏少量输入的部件或系统。\n方程形式为 ： $$\\frac{dx}{dt} = -Ax + C$$ 其中C是输入，A是“泄漏”速率。\n   $F(s)$ 表示描述一组神经元细胞的放电速率，每个细胞的放电速率的实值为 $s$ 。在每个时刻，所有细胞都接收当前值 $f(\\tau)$ 的输入，并遵循下面这个微分方程更新数值： $$\\frac{d F(s,\\tau)}{d \\tau} = \\alpha(\\tau)[-s F(s,\\tau) + f(\\tau)]$$\n $F$ 是细胞索引和时间的函数 $\\alpha(\\tau)$ 对所有细胞都是相同的，我理解是对时间的编码  先使 $\\alpha(\\tau) = 1$，解得： $$F(s,\\tau)=\\int^\\tau_{-\\infin} e^{-s(\\tau-\\tau')}f(\\tau')d\\tau'$$\n与拉普拉斯变换的标准定义比较，发现 $F$ (像函数) 是只具有实系数的 $f(\\tau' \u0026lt; \\tau)$ （原函数）的拉普拉斯变换，记为： $$F = \\mathscr{L}f$$\n 变化成$F$不会丢失任何关于$f$的信息 特定函数 $f$ 的拉普拉斯变换导致一个唯一的 $F$ 可逆，恢复$f$相当于找到一种神经上可行的方法来做拉普拉斯逆变换  每个细胞将代表不同的延迟，时间活动的历史将被跨细胞记载。因为有限的细胞，宽调谐曲线是更可取的。\n然后作者定义了一个近似拉普拉斯变化的线性算子 $L^{-1}_k$ ，用于描述构成中间表示 $F$ 的细胞和 构成重建 $\\widetilde{f}$ 的细胞之间的连接组： $$\\widetilde{f} \\equiv L^{-1}_k F(s) \\equiv C_k s^{k+1} \\frac{d^k}{ds^k} F(s)$$\n $C_k$ 是取决于 k 的常数，简单地设置了发放的整体比例，确保重建的符号与输入函数的符号对应。 $\\frac{d^k}{ds^k}F(s)$ 是造成 $\\widetilde{f}$ 状态随时间变化的因子，是 $F(s)$ 对 $s$ 的 $k$ 次导数。一个简单的带侧抑制的前馈电路足以近似二阶导数。高阶导数可以通过串联计算低阶导数的电路来计算。例如，四阶导数就是二阶导数的二阶导数。 当 k 趋于无穷，$L^{-1}_k$就是拉普拉斯逆变换  过去 $\\tau_0$ 一个非0输入，则 $F(s) = e^{-s\\tau_0}$\n \n当输入给两个方波脉冲，$F(s)$像充电电容器呈指数增长和指数衰减，$\\widetilde{f}(\\tau^*)$ 的细胞不会对刺激立即反应，在出现非零值后一个特征时间发放，一个瞬时刺激导致一系列细胞在它被呈现后激活。（虚线是不同 s 值）  当$\\alpha(\\tau)=1$时，$F(s)$和$\\widetilde{f}(\\tau^*)$  上面这个图表现出编码更久远时间（延迟长）的细胞的发放分布更广的特性，能用于描述记忆和时间的行为（Gallistel and Gibbon，2000；Brown et al.，2007；Chater and Brown，2008）。\n “The spread is deterministic and is a commitment to representing the history with decreasing accuracy.” 我个人理解是作者想表达在他这个框架的意义上，更早记忆的具体时间更难确定不是因为时间长了各种神经活动产生的噪声干扰，而是时间记忆方式本身带来的。它就是精确的，度量的最小精度会不断下降。\n 结果 纯粹的时间表示  能够确认两个定性预测\n CA1中的时间细胞，对于延迟期较晚激发的细胞，发放的时间分布会发散。 时间细胞发放的时间分布是不对称的。   记忆任务的延迟期间（大鼠必须记住两个物体中的哪一个被呈现，以便在延迟后呈现的一对气味中进行适当的选择并获得奖励），类似于place cells，(MacDonald et al. 2011) 观察到海马一些神经元（在这里称时间细胞）在一个特定的延迟时间响应，并且区分了在延迟之前经历的对象。\n 实验观察到的海马时间细胞与 $\\widetilde{f}(\\tau^*)$ 的比较 \n上图a表示实验观察到的时间细胞随着延迟和不对称性而表现出越来越大的分布，展示了两个代表性细胞的发放数据，红线是分布的估计值，中间粗蓝线是峰值，两侧细蓝线是半高区域估值，看出后半段半高区域范围大于前半段半高区域。\n上图b表示来自实验数据延迟期间的整体相似性。 上图c显示的$\\widetilde{f}(\\tau^*)$ 构建的整体相似性。\n时间细胞的时间分布也呈正偏态，在分析的63个细胞中，有45个细胞的后缘大于前缘（下图c）。这种效应并不是由于切断延迟间隔而产生的边缘效应：即使将注意力限制在间隔开始和结束1000ms内的细胞上，我们发现31/39呈正偏，比例明显大于偶然性（下图d）。\n \n收获和想法  收获\n  一些数学知识，比如拉普拉斯变换 为了看懂这文章补的海马体相关神经科学知识 学习了 Leaky Integrate-and-Fire (LIF) 模型 最重要的，给我带来了认知主观时间、空间神经表征的一个新视角，就很像存储记忆的同时会通过海马的一些细胞加时间戳。   想法\n  懂了但没完全懂，感觉论文写的很杂乱，很多东西交代不清楚，导致读起来很累和痛苦。 相较于place cell、grid cell，我对文章引用的实验所说的“time cell”持保留意见。 作为数学框架，还是很单薄。  额外参考  4.1-Leaky Integrate-and-Fire (LIF) 模型 Leaky integrator \u0026ndash;From HandWiki 拉普拉斯变换 \u0026ndash; 小时百科 从另一个角度看拉普拉斯变换 大脑是如何为往事铺上时间线的 海马启发下的类脑人工智能  ","date":"2022-10-05T03:29:26+08:00","permalink":"https://deathsprout.github.io/p/a-unified-mathematical-framework-for-coding-time-space-and-sequences-in-the-hippocampal-region/","title":"A Unified Mathematical Framework for Coding Time, Space, and Sequences in the Hippocampal Region"},{"content":"This is a note  本文同时是 神经科学22-23秋季 郝荣乔老师课程作业.   前情提要 先简要说明为什么我会选择这样的一篇文章，要求的范围是视觉与美学（visual procession、aesthetic、visual cognition、art），同时这个时期，大量的基于Diffusion Model的文字-图片生成模型工作出现在人们视野中并引起广泛讨论（特别是商业美术从业者，很多朋友因此改变了职业规划）。 从GAN 的时代我就非常关注图像生成模型（爱好画画的缘故），近一个月来的进展给我带来了极大的震撼，我研究并尝试了Midjourney (托管在discord上)、Stable Diffusion、DreamStudio、 百度的文心、Waifu Diffusion、NovelAiDiffusion。\n下面列出了几张我使用这些Ai生成的图片。\n Midjourney   Waifu Diffusion   Waifu Diffusion   DreamStudio   文心 \n国庆发现的NovelAiDiffusion，和WD一样也是基于SD，使用要10$/月订阅，所以我拿了别人生成的三张图。非常nb，在我这儿是能通过图灵测试的，我能联想到几个画风类似的画师，但我并不能分辨出来区别。（这个AI当天引起了我们一小撮人关于未来的激烈讨论（失业淘汰、自由画师赚钱吃饭、版权、未来职业形态等等），在惊叹之余都挺受打击，纯粹的画工门槛已经没有意义）\n NovelAiDiffusion   NovelAiDiffusion   NovelAiDiffusion \n大概几周前我简单的用PS组合了一张艺术摄影和用AI生成的平面素材，制作了下面这张海报。很酷不是么，AI终究会给美术艺术带来第二次摄影冲击， 所以我打算阅读DDPM模型的重要文章。\n \n文章背景 （去噪）扩散模型有两个过程，前向过程（diffusion，不断添加高斯噪声）和反向过程（reverse，去噪过程，用来生成数据），是一个参数化的马尔可夫链。  \n扩散过程 扩散过程是指的对数据逐渐增加高斯噪音直至数据变成随机噪音的过程。   对于原始数据 $x_0$ ,扩散过程的每一步都是对上一步得到的数据以如下方式增加高斯噪音 ($q(x_t|x_{t-1})$是变分推理引入的用于近似后验分布的分布，可以看我之前的笔记变分自编码器) ：   $\\beta_t$ 是第$t$ 步采用的方差，DDPM会预先定义好一个线性的方差序列。因此扩散过程可以直接基于$x_0$对任意 $t$ 步的$x_t \\thicksim q(x_t|x_0)$进行采样得到：\n $\\alpha_t = 1-\\beta_t \\qquad \\overline{\\alpha}t = \\prod^t{i=1}\\alpha_i$ \n$$q(x_t|x_{0}) = \\mathcal{N}(x_{t};\\sqrt{\\overline{\\alpha_t}}x_{0},(1-\\overline{\\alpha}_t) I)$$\n反向过程 反向是从一个随机噪音逐渐去噪以生成一个真实的样本。\n这一过程需要我们知道每一步的真实分布 $q(x_{t-1}|x_{t})$，似然地估计这个分布 $p_\\theta(x_{t-1}|x_{t})$ 就需要用到整个训练样本，由一系列神经网络参数化的高斯分布 ( 均值和方差由训练网络$\\mu_\\theta(x_t,t)$和$\\Sigma_\\theta(x_t,t)$ 给出 ) 组成。\n  在得知$x_0$时估计条件后验分布$q(x_{t-1}|x_{t},x_0)$: $$q(x_{t-1}|x_{t},x_0) = \\mathcal{N}(x_{t-1};\\widetilde{\\mu}(x_t,x_0),\\widetilde{\\beta}_t I)$$  \n推导  根据贝叶斯公式： $$ q(x_{t-1}|x_t,x_0) = q(x_t|x_{t-1},x_0) \\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}$$ 由扩散过程知道 $q(x_t|x_{t-1},x_0) = q(x_t|x_{t-1}) = \\mathcal{N}(x_{t};\\sqrt{1-\\beta_t}x_{t-1},\\beta_t I) $ , 也得出了:\n$$q(x_t|x_{0}) = \\mathcal{N}(x_{t};\\sqrt{\\bar{\\alpha}_t}x_{0},(1-\\bar{\\alpha}_t) I)$$\n$$q(x_{t-1}|x_{0}) = \\mathcal{N}(x_{t-1};\\sqrt{\\bar{\\alpha}_{t-1}}x_{0},(1-\\bar{\\alpha}_{t-1}) I)$$\n$$\\therefore q(x_{t-1}|x_t,x_0) \\varpropto exp(-\\frac{1}{2} (\\frac{(x_t-\\sqrt{\\alpha_t} x_{t-1})^2}{\\beta_t} +\\frac{(x_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1} } x_{0})^2}{1-\\bar{\\alpha}_{t-1} } - \\frac{(x_{t}-\\sqrt{\\bar{\\alpha}_t}x_0)^2}{1-\\bar{\\alpha}_{t}}))$$\n 能化成$x_{t-1}$的二次式  上面括号最里面等于 $\\frac{(x_{t-1}-\\widetilde{\\mu}_t(x_t,x_0))^2}{\\widetilde{\\beta}_t}$, 就能得到这两个值的表示。  \n   优化目标 这个设置非常像VAE，通过优化负对数似然的变分下界（ELBO）来进行训练 （最大化log形式的产生真实数据的可能性( $log p_\\theta(x)$), 最小化真实后验分布和估计的后验分布的差异(KL散度)），其训练目标是 $-L_{VLB}$:\n$$L = -L_{VLB}= \\mathbb{E}_{q(x_{1:T}|x_0)}[-log\\frac{p_\\theta(x_{0:T})}{q(x_{1:T}|x_0)}] = \\mathbb{E}_{q(x_{1:T}|x_0)}[log\\frac{q(x_{1:T}|x_0)}{p_\\theta(x_{0:T})}] $$\n \n这样就得到了论文第三页怼脸上的公式5.\n贡献 就是说上面那些，是之前的文章（概念最早是在2015年）提出 Diffusion models 所带来的公式，其网络架构和训练方法也比较简陋。这两年的大部分扩散模型相关工作都是基于两个模型：DDPM和DDIM。\n那么这 T+1 个优化目标要怎么算呢？\n  $L_T$ （最后得到的噪音的分布和先验分布的KL散度）: 因为将前向过程的方差固定了，这个KL散度没有训练参数，在学习过程该项为常量，忽略。\n  $L_0$ （原始数据重建）: 这里用了一个编码器来计算（文章认为这部分简单，留给未来的工作优化）\n  $L_{1:T-1}$ (估计分布 $p_\\theta(x_{t-1}|x_{t})$ 和真实后验分布 $q(x_{t-1}|x_{t},x_0)$ 的KL散度) : 希望估计的去噪过程和依赖真实数据的去噪过程近似一致,\n 首先是会设置固定方差 $\\Sigma_\\theta(x_t,t) = \\sigma^2_t \\quad x_0=\\mathcal{N}(0,I)$ , 根据文章的实验 $\\sigma^2_t = \\beta_t$ 还是 $\\sigma^2_t = \\widetilde{\\beta}_t$ 结果都差不多。  两个高斯分布的KL散度计算结果，推导可见  \n   \n原本这个参数在$\\mu$底下，预测的是均值，文章重新参数化方程 $$q(x_t|x_{0}) = \\mathcal{N}(x_{t};\\sqrt{\\overline{\\alpha_t}}x_{0},(1-\\overline{\\alpha}_t) I) \\to x_t(x_0,\\epsilon_t) = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t \\quad \\epsilon_t \\thicksim \\mathcal{N}(0,I)$$\n将 $x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(x_t-\\sqrt{1-\\bar{\\alpha}_t}\\epsilon)$ 代入 $\\widetilde{\\mu}_t(x_t,x_0)$ 得到：  \n因为 $x_t$ 在训练时可以作为输入，所以我们可以重新参数化高斯噪声项 $\\epsilon_\\theta(x_t,t)$，让它从步$t$时的输入$x_t$学得，这样就从原来的预测均值变为预测高斯噪声。  \n代入优化目标可得（上面的$L_{t-1}$ 和下面的 $L_t$ 意义相同，都是代表当 $t \\neq 0 、T$ ）:  \nDDPM近一步对上述目标进行了简化，去掉了权重系数  \n   \n代码实现  暂时放放\n 参考   文献\n Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in Neural Information Processing Systems, 2020, 33: 6840-6851. （原文链接） Yang L, Zhang Z, Hong S. Diffusion models: A comprehensive survey of methods and applications[J]. arXiv preprint arXiv:2209.00796, 2022.    A New Look at Denoising Diffusion Models（1）\n  Denoising Diffusion Probabilistic Model(DDPM)\n  扩散模型 Diffusion Model \u0026ndash;b站\n  扩散模型之DDPM\n  变分推理（variational inference）\n  变分自编码器\n  What are Diffusion Models?\n  Generative Modeling by Estimating Gradients of the Data Distribution\n  两个多元正态分布的KL散度、巴氏距离和W距离\n  ","date":"2022-10-03T15:22:20+08:00","image":"https://pic.jitudisk.com/public/2022/10/03/9b7240cf7ec88.png","permalink":"https://deathsprout.github.io/p/denoising-diffusion-probabilistic-models/","title":"Denoising Diffusion Probabilistic Models"},{"content":"构建空间认知地图的机制是一种通用编码机制的实例，能够涵盖任何领域的抽象认知地图。\nCognitive map  Tolman在1948年提出，空间或者地图的概念在小鼠的大脑里是存在的， 成为其导航学习的基础。\n Place cell ：不停的经过某个同样的地点，同一个细胞会放电 Grid cell : 感受野对空间进行周期性的放电，它可以把一个二维平面表现成一个密集堆积的六边形结构， 不同的grid-cell具有不同的空间周期。（对空间里的一个狄拉克函数（一个空间质点的表示）做傅里叶变换会得到一系列不同周期频率的波函数， 反过来这群函数或许可以作为一组表达不同物体位置的基函数。）       The cognitive mapping problem  学习或关注适当的抽象是认知地图的核心问题。   强化学习状态空间，图与图表示 认知图的生成问题与 RL 中的状态空间生成问题是一样的  \n 参考  深度强化学习落地方法论（5）—— 状态空间篇 海马启发下的类脑人工智能  ","date":"2022-09-30T13:52:50+08:00","permalink":"https://deathsprout.github.io/p/how-to-build-a-cognitive-map/","title":"How to build a cognitive map"},{"content":"This is a note  本文同时是 系统与计算神经科学22-23秋季 余山老师课程第一次作业. || 原文链接   研究意义 研究在不同层次上进行对大脑的仿真，模拟了一个大规模的丘脑皮质网络（虽然没有模拟丘脑以外的皮层下结构，但模拟了脑干神经调节，包括多巴胺能奖赏系统和胆碱能激活系统，神经放电时间依赖的突触可塑性只明确建模了树突STDP）, 这个模型能在没有任何外部输入源的情况下自我维持，并且自发产生节律和波。 作为自下而上的仿真工作，这项研究加深了我们对突触和神经元过程如何相互作用以产生大脑的集体行为的理解。\n该模型能呈现出一些与当前生物大脑研究中相符的特性，故而进一步开发更详细、更完整的大脑模型能够作为很好的工具，通过模拟来帮助研究药理、结构扰动、神经系统疾病等对大脑神经动力学的影响。\n方法 模型 1.宏观解剖  用人脑核磁共振成像得到的弥散张量成像（DTI）数据，作用是将模型神经元细胞体分配到合适的位置，使模型能反映人脑脑沟和脑回的折叠皮层结构。 DTI数据用“TensorLine”算法形成白质束（小纤维束因分辨率被遗漏），白质束跨区域连接神经元。 为了使每平方毫米皮质表面的神经元密度足够大，模型的空间尺度减少了4倍。  2.微观解剖 下图给出了模拟灰质微电路的摘要，基于Binzegger等人对猫视觉皮层的详细重建研究。\n \n3.树突分枝 每个神经元都有一个胞体室和许多树突室，初始化过程动态确定每个神经元的树突状室的确切数量，每个室突触数少于40。\n隔室用于模拟突触前神经元在树突中诱发局部EPSP，电位就近叠加引发spike，不同隔室的突触后电位引发效果差。在同一室中三个或四个突触的同时发射可以导致局部树突动作电位，然后可以传播到体细胞以引起尖峰或爆发反应。\n4.神经动力学 使用作者Izhikevich提出的唯象模型来模拟每个神经元和每个树突室的发放动力学，不是像HH模型那样模拟所有离子电流。  对体外记录（左栏）和模拟（右栏）的四种代表性放电模式进行比较。 \n5.短时突触可塑性  短时突触抑制和易化由一维方程建模，左侧是短时抑制；右侧短时易化。红色曲线是建模结果，相叠的灰色曲线是测量结果。 \n6.树突STDP 模型中每个突触的电导（权重）的长期变化是根据刺激时间依赖性可塑性（STDP）模拟的。\nSTDP：指刺激两个神经元时, 刺激（spike）突触前神经元的时间早于刺激突触后神经元的时间时, 可以得到突触后电流的增强; 当突触前神经元的刺激晚于突触后神经元的刺激时, 突触后电流将会受到抑制。当两个刺激间时间差值发生改变时也能够影响突触后电流的增强效果。（被认为是学习记忆的基本机制）\n7.计算机模拟  程序：C语言 处理器： 60个 3GHz 1.5GB内存的 Beowulf 集群 规模 ： 100万神经元 、数千万神经元室、 近5亿突触  结果 自发活动   文章在 t=0 时引入一些随机（种子）发放来启动网络，不论随机种子数量、突触链接初始强度或网络大小，网络活动都会很快消失。 作者以体内体外都被观察到的微型突触后电位（mPSPs，“minis”）为原型，用刺激性随机输入喂神经元，赋予了神经元一个自发（泊松分布）的突触发放，防止网络沉默。\n30分钟后关闭minis也不会导致网络沉默，作者认为是这个过程的STDP微调了突触连通性，以允许足够的神经元间动作电位来维持全局活动。关闭minis30分钟后（1h），将此刻的瞬间状态作为大多数后续模拟的初始状态。\n混沌系统  初始条件仅差一个spike的两个模拟，会在半秒内发散(上)，（下）显示两个模拟间不相同的神经元发放  单个神经元级别的改变对整体活动影响很大，对初始条件的小扰动的敏感性高\n大脑节律和波  在模型中传播的波，红（黑）点对应兴奋性（抑制性）的 spike，原文附件有一个视频，推荐去观看 \n 同一类型神经元在不同位置放电频率可能会有很大差距，文章提到FS型篮（basket）细胞平均放电频率在2/3、4层 7Hz,5层大于20Hz，6层8Hz。 强烈的局部伽马节律在大的尺度被抵消，与EEG和MEG记录的伽马节律弱于颅内脑电的实验观察一致。 整个网络出现显著的低频活动，主要在 $\\delta(1-3Hz)$ （哺乳动物睡眠状态的典型波段）和 $\\alpha(~10Hz)$（人类皮层闲置期间的典型波段）。  收获和想法  收获:\n   新的知识：\n DTI 的新用途。 不同树突室诱发的EPSP，难以叠加引发突触后电位。 burst：一种放电模式的称呼 ，神经元在短时间内多次放电，然后进入静止期。 同一种神经细胞原来会在不同区域表现出不同的发放频率。 微型突触后电位的存在。 系统对初始条件小扰动的敏感性，即“蝴蝶效应”，是混沌系统的主要特征。    研究思路：\n 作为复杂的仿真，为了减轻运算，可以使用唯象的模型代替过于细致的模型。     想法（感慨）：\n  第一次接触这方面的工作，特别钦佩研究者能进行如此复杂的多层次仿真，同时能把这样的工作写的很易读。 仿真实现对应的C程序工作量肯定很大，很好奇这类工程是如何组织开展的，又是怎样管理的。 如果说这个仿真研究中，脑网络需要一定的微型突触后电位作为刺激随机输入防止网络沉默。那么在人类大脑发育的过程中，随着脑神经元数量增多，神经连接构成足够复杂的网络，是否也会有脑网络“点火”的一瞬间？因为我觉得脑网络应该是从沉寂自发点火再维持在一个临界的状态，不是研究中设置随机种启动再进入自我维持状态，这个阶段中或许还有个特殊的过程。  ","date":"2022-09-18T13:59:42+08:00","image":"https://pic.jitudisk.com/public/2022/09/18/0061f95634cbf.png","permalink":"https://deathsprout.github.io/p/large-scale-model-of-mammalian-thalamocortical-systems/","title":"Large-scale model of mammalian thalamocortical systems"},{"content":"定位  1662529991758.png \n dorsal 背侧 rostral 吻侧 ventral 腹侧 caudal 尾侧  脑干和小脑处命名有90度旋转。\n 1662530390664.png \n horizontal （axial transverses） coronal plane 冠状面 sagittal plane 矢状面  层次关系 神经系统\n 中枢神经系统 CNS  脑 Brain  端脑 Telencephalon 间脑 Diencephalon 小脑 Cerebellum 脑干 Brain stem  中脑 Midbrain 脑桥 Pons 延髓 Medulla oblongata     脊髓 Spinal cord   外周神经系统 Peripheral NS  运动神经元 Motor Neurons  躯体神经系统 自主神经系统  交感神经 (Sympathetic part) 副交感神经(Parasympathetic part) 肠部神经 Enteric division     感觉神经元 Sensory Neurons     1662530684306.png \n材质  CNS\n  Gray matter 灰质 Cortex 皮质 White matter 白质 Medulla 髓质 Nucleus 神经核 Fascicules 纤维束 Reticular formation 网状结构   PNS\n  Ganglion 神经节 Nerve 神经  逐一 端脑 Telencephalon 由左右半球(cerebral hemisphere)及胼胝体(corpus callosum)组成。\n半球由浅入深\n 大脑皮质 cerebral cortex 髓质 medulla 基底核 basal nuclei 侧脑室 lateral ventricle   1662530684306.png \n脑叶   The Frontal Lobe 前额叶：\n Premotor Cortex Motor cortex   病变产生痉挛性对侧无力     Prefrontal cortex   人类执行功能最重要的领域 个性、情绪、驱动力、背景、计算、工作记忆和注意力 对心理功能和疾病至关重要       The Parietal Lobe 顶叶\n   sensation ,body position, speech and language + Somatosensory cortex\n  The Temporal Lobe   Memory,hearing,organisation and understanding language + Wernicke’s area (语言理解)\n   Medial Temporal Lobe（MTL）内侧颞叶  1662533663513.png \n  The Occipital Lobe 枕叶\n Primary visual cortex    Insula Lobe 岛叶\n  Limbic System 边缘系统\n  脑裂 基底核 Basal Ganglia  1662534101156.png \n白质 目前无法看到全部纤维束\n Association fibers 联络纤维   在同一半球内的脑回之间\n  Commissural fibers 连合纤维   左右半球之间\n  Projection fibers 投射纤维   连接皮层和大脑下部和脊髓，它们包括上升和下降纤维\n 分类 宏观尺度脑图谱分类  1662532613398.png \n参考  国科大自动化所课程 | 脑网络组学第二讲 | 讲者：樊令仲 （临床医学背景）   ","date":"2022-09-07T13:25:18+08:00","image":"https://pic.jitudisk.com/public/2022/09/07/12fa725f983bb.png","permalink":"https://deathsprout.github.io/p/%E7%A5%9E%E7%BB%8F%E8%A7%A3%E5%89%96%E5%AD%A6%E7%AC%94%E8%AE%B0/","title":"神经解剖学笔记"},{"content":"The Basics  A graph is a pair $G=(V,E)$ , $ G $ are vertics, and $E$ means edges, $E$ are 2-element subsets of $V$.\nA vertices $v$ is incident with an edge $e$ if $v\\in e$,two vertices incident with an edge are endvertices or ends.The set of all the edges in $E$ at a vertices $v$ is denoted by $E(v)$.\nSome concept:\n  graph\u0026rsquo;s order written as $|G|$, means the number of vertices. The number of edge denoted by $||G||$. empty graph $(\\emptyset,\\emptyset)$ simply write $\\emptyset$, A graph of order 0 or 1 is called trivial.\n  adjacent or neighbours : vertices have an edge ; edge have an end in common\n  complete : all vertices of a graph are pairwise adjacent. A complete graph on $n$ vertices is a $K^n$, example $K^3$ is a triangle(三角形).\n  independent : non-adjacent; for a set, if no two of its elements are adjacent.\n  isomorphic ， write $G \\simeq G^\\prime$ (usually like write $G = G^\\prime$), if two graphs exists a bijection (双射) $$\\varphi : V \\to V^\\prime ,\\text{with} ,, xy\\in E \\Leftrightarrow \\varphi (x)\\varphi(y) \\in E^\\prime \\quad \\forall x,y\\in V$$\nso the map $\\varphi$ called isomorphism , if $G = G^\\prime$ is automorphism (自同态（形）).\n  graph property : A class of graphs that is closed under isomorphism.\n  graph invariant : A map which assigns equal values to isomorphic graphs. (在同形图上不变的量，比如节点数量和边数量)\n  Set operation :\n  Union : $G \\cup G^\\prime := (V\\cup V^\\prime,E\\cup E^\\prime)$, difference similarly, and if $G\\cap G^\\prime = \\emptyset$, then they are disjoint.\n  induced subgraph : $G \\subseteq G^\\prime$ and $G^\\prime $ has all the edge $xy\\in E$ with $x,y \\in V^\\prime$. write $G^\\prime =: G[V^\\prime]$. (只少了点的子集，包括的点同时包括所有对应边)\n  spanning subgraph : $G \\subseteq G^\\prime$ and $V^\\prime$ spans all of $G$, i.e. if $V^\\prime=V$. (点都有的子图)\n  $G - U$ is $G [V\\setminus U]$, $U$ is any set of vertices. Similarly, $G+F := (V,E\\cup F)$.\n  $G* G^\\prime$ : $G$ and $G^\\prime$ is disjoint, and joining all vertices with each other. For example, $K^2 * K^3 = K^5 $.\n  complement : write is $\\overline{G}$, $\\overline{G} = G ([V]^2 \\setminus E ) $\n  line graph : $L(G)$ of $G$ is the graph on $E$, which edges as vertices and adjacent edges as edges. (边视为顶点的新图)\n  degree of a vertex The set of neighbours of a vertex $v$ in $G$ is denoted by $N_G(v)$ or $N(v)$, for a set $U\\subseteq V$, $N(U)$ means the neighbours in $V\\setminus U$ for all vertices in $U$. The degree $d(v)$ is the number $|E(v)|$ of edges at $v$.\nisolated meaning a vertex degree is 0, $\\delta(G)$ and $\\Delta(G)$ is the minimum degree and maximum degree of $v\\in V$.\n$d(G)$ is the average degree of $G$, sometimes directly express as $\\varepsilon(G) := |E|/|V|$, and $\\varepsilon(G) = \\frac{1}{2} d(G)$.\n $k$-regular : all the vertices of $G$ have same degree $k$. cubic is a 3-regular graph.  Proposition : The number of vertics of odd degree in a graph is always even\nPaths and cycles A path is a non-empty graph $P=(V,E)$ of the form: $$V={x_0,x_1,\u0026hellip;\u0026hellip;, x_k} \\quad E={x_0x_1,\u0026hellip;\u0026hellip;,x_{k-1}x_k}$$\nThe number of edges of a path is its length, denoted by $P^k$. $P^0 = K^1$.\n A-B path : path $P = x_0\u0026hellip;x_k$ and $V(P)\\cap A={x_0} \\quad V(P)\\cup B={x_k}$ independent paths : none of them contains an inner vertex of another.(互不包含中间节点) H-path : $P$ two ends in graph H  cycle : $P = x_0\u0026hellip;x_{k-1}$ is a path and $k\\ge 3$, the graph $C^k := P + x_{k-1}x_0$.\n girth $g(G)$ : The minimum length cycle of $G$ .(no cycle is zero) circumference : The maximum length cycle of $G$.(no cycle is $\\infty$) chord : two vertices of a cycle joining a edge but not be the edges of the cycle induced cycles : The cycle is a subgraph of other cycle diam $G$ : The greatest distance between any two vertices of $G$  Proposition : Every graph containing a cycle satisfies $g(G)\\le 2 ,\\text{diam} ,G + 1$\n central : A vertex in $G$ its greatest distance with any other vertex is smallest. And this distance is the radius of $G$, denoted by $rad G$  $$rad, G \\le diam ,G \\le 2 , rad , G$$\nCorollary : If $\\delta (G) \\ge 3$ then $g(G) \u0026lt; 2 log|G|$.\n walk : If the vertices in a walk are all distinct, is a path. connected : any two of vertices are linked by a path in graph $G$ component : 图的一个最大联通子图 separates : 如果所有A-B路径都需要经过X的点或边，称X分割了A、B。 edges of X, deifnition of bridge. two vertics is a cutvertex.  Just Words   nuisanse : 麻烦事 disregard ：不理会      ","date":"2022-08-10T15:32:56+08:00","image":"https://s2.loli.net/2022/08/10/kohwepMSim9D1xs.png","permalink":"https://deathsprout.github.io/p/%E5%9B%BE%E8%AE%BA%E5%AD%A6%E4%B9%A0/","title":"图论学习"},{"content":"\n怎么实现  APlayer 是一个简洁漂亮、功能强大的 Html5 音乐播放器 MetingJS 是为 APlayer 添加网易云、QQ音乐等支持的插件  需要在 \\layouts\\partials\\head\\head.html里添加\n\u0026lt;!-- require APlayer --\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- require MetingJS --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 如何使用 从已有音乐播放器引用 这种方式非常简单，比如下面代码的效果就是未闻花名李让的口琴曲子\n\u0026lt;meting-js server = \u0026#34;netease\u0026#34; type=\u0026#34;song\u0026#34; id=\u0026#34;31209775\u0026#34; list-folded=true\u0026gt;\u0026lt;/meting-js\u0026gt; \nserver = \u0026#34;netease\u0026#34; # 表示音乐来自网易云 type = \u0026#34;song\u0026#34; # 表示歌曲 id = \u0026#34;xxxxx\u0026#34; # 是歌曲分享时复制链接里的id ，这里 https://music.163.com/song?id=31209775\u0026amp;userid=412281089 就是 song?id= “31209775”  server 指定调用的 API ，可选 netease, tencent, kugou, xiami, baidu ，分别对应网易云音乐、QQ音乐、酷狗音乐、虾米音乐、百度音乐 type 指定调用类型，可选 song, playlist, album, search, artist ，分别对应单曲、歌单、专辑、搜索结果、艺术家 id 指定调用的 id ，一般可以在地址栏中找到  下面是我2020年网易云生成的歌单，可以看到效果如下： \n自己编写的音频怎么分享？ 稍微麻烦一些，个人理解有两种方式\n 一是申请各种音乐网站的音乐人，这样就有权在上面发自己的音频，流程就和上面一样了。好处是引用代码短，缺点是成为音乐人要签一些他们的合同，而且是有审核的，至少有不错的东西提交审核，时间也可能比较长。（对于我这种菜鸡加懒汉，写一个正常长度的曲子属于奢望 😤 ） 二是将音频托管到类似图床的东西里， 我这里用的是Gimhoy音乐盘，缺点是加载慢，用起来代码比较长（因为大部分复制粘贴所以还凑合）     const ap = new APlayer({ container: document.getElementById('aplayer'), fixed: false, //是否附着页面底部，否为false autoplay: true, //是否自动播放，否为false,移动端不能生效 volume: 0.6, //初始音量（0~1） lrcType: 3, //歌词模式（1、HTML模式 2、js模式 3、lrc文件模式） mutex: true, //互斥模式：阻止多个播放器同时播放，当前播放器播放时暂停其他播放器 order: 'random', //音频循环顺序（'list'：顺序, 'random'：随机） preload: 'auto', //预加载（'none'：不预加载, 'metadata'：元数据, 'auto'：自动） listFolded: false, //列表默认折叠，开启为true theme: '#ee8243', //主题颜色 audio: [{ name: '低唤醒', //歌曲名称 artist: 'DeathSprout', //歌曲作者 url: \"https://link.jscdn.cn/sharepoint/aHR0cHM6Ly8xZHJpdi1teS5zaGFyZXBvaW50LmNvbS86dTovZy9wZXJzb25hbC9zdG9yXzFkcml2X29ubWljcm9zb2Z0X2NvbS9FUkp3V1AtU2NxcE91dktveG1VNUhDc0JJQmwyT3lOU19pMUl3anBERWNwbE5n.m4a\" , //歌曲源文件地址 cover: \"https://s2.loli.net/2022/05/13/Y84ArkwgyR3vK9O.png\", //歌曲封面地址 lrc: 'lrc.lrc', //歌曲的歌词文件 theme: '#eeeeee' //主题颜色（优先） }] });  对应的代码，可以自定义音乐标题、封面啥的。（本人渣作 😳 ，属于是猩猩在键盘上反复敲）\n\u0026lt;div id=\u0026#34;aplayer\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;APlayer.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; const ap = new APlayer({ container: document.getElementById(\u0026#39;aplayer\u0026#39;), fixed: false, //是否附着页面底部，否为false  autoplay: true, //是否自动播放，否为false,移动端不能生效  volume: 0.6, //初始音量（0~1）  lrcType: 3, //歌词模式（1、HTML模式 2、js模式 3、lrc文件模式）  mutex: true, //互斥模式：阻止多个播放器同时播放，当前播放器播放时暂停其他播放器  order: \u0026#39;random\u0026#39;, //音频循环顺序（\u0026#39;list\u0026#39;：顺序, \u0026#39;random\u0026#39;：随机）  preload: \u0026#39;auto\u0026#39;, //预加载（\u0026#39;none\u0026#39;：不预加载, \u0026#39;metadata\u0026#39;：元数据, \u0026#39;auto\u0026#39;：自动）  listFolded: false, //列表默认折叠，开启为true  theme: \u0026#39;#ee8243\u0026#39;, //主题颜色  audio: [{ name: \u0026#39;低唤醒\u0026#39;, //歌曲名称  artist: \u0026#39;DeathSprout\u0026#39;, //歌曲作者  url: \u0026#34;https://link.jscdn.cn/sharepoint/aHR0cHM6Ly8xZHJpdi1teS5zaGFyZXBvaW50LmNvbS86dTovZy9wZXJzb25hbC9zdG9yXzFkcml2X29ubWljcm9zb2Z0X2NvbS9FUkp3V1AtU2NxcE91dktveG1VNUhDc0JJQmwyT3lOU19pMUl3anBERWNwbE5n.m4a\u0026#34; , //歌曲源文件地址  cover: \u0026#39;cover.jpg\u0026#39;, //歌曲封面地址  lrc: \u0026#39;lrc.lrc\u0026#39;, //歌曲的歌词文件  theme: \u0026#39;#eeeeee\u0026#39; //主题颜色（优先）  }] }); \u0026lt;/script\u0026gt; 参考   APlayer \u0026amp; MetingJS 音乐播放器使用指南\n  记录一次折腾Aplayer播放器\n  ","date":"2022-05-07T18:07:35+08:00","permalink":"https://deathsprout.github.io/p/blog%E5%B5%8C%E5%85%A5%E9%9F%B3%E4%B9%90%E5%8F%8A%E4%B8%AA%E4%BA%BA%E5%88%9B%E4%BD%9C%E5%88%86%E4%BA%AB/","title":"Blog嵌入音乐及个人创作分享"},{"content":"  #the-canvas { border: 1px solid black; direction: ltr; width: 100%; height: auto; display: none; } #paginator { display: none; text-align: center; margin-bottom: 10px; } #loadingWrapper { display: none; justify-content: center; align-items: center; width: 100%; height: 350px; } #loading { display: inline-block; width: 50px; height: 50px; border: 3px solid #d2d0d0;; border-radius: 50%; border-top-color: #383838; animation: spin 1s ease-in-out infinite; -webkit-animation: spin 1s ease-in-out infinite; } @keyframes spin { to { -webkit-transform: rotate(360deg); } } @-webkit-keyframes spin { to { -webkit-transform: rotate(360deg); } }  Previous Next \u0026nbsp; \u0026nbsp; Page:  /       window.onload = function() { var url = \"https:\\/\\/deathsprout.github.io\\/\" + 'post\\/实习报告\\/实习报告ya.pdf'; var hidePaginator = \"\" === \"true\"; var hideLoader = \"\" === \"true\"; var selectedPageNum = parseInt(\"\") || 1; var pdfjsLib = window['pdfjs-dist/build/pdf']; pdfjsLib.GlobalWorkerOptions.workerSrc = \"https:\\/\\/deathsprout.github.io\\/\" + '/js/pdf-js/build/pdf.worker.js'; var pdfDoc = null, pageNum = selectedPageNum, pageRendering = false, pageNumPending = null, scale = 3, canvas = document.getElementById('the-canvas'), ctx = canvas.getContext('2d'), paginator = document.getElementById(\"paginator\"), loadingWrapper = document.getElementById('loadingWrapper'); showPaginator(); showLoader(); function renderPage(num) { pageRendering = true; pdfDoc.getPage(num).then(function(page) { var viewport = page.getViewport({scale: scale}); canvas.height = viewport.height; canvas.width = viewport.width; var renderContext = { canvasContext: ctx, viewport: viewport }; var renderTask = page.render(renderContext); renderTask.promise.then(function() { pageRendering = false; showContent(); if (pageNumPending !== null) { renderPage(pageNumPending); pageNumPending = null; } }); }); document.getElementById('page_num').textContent = num; } function showContent() { loadingWrapper.style.display = 'none'; canvas.style.display = 'block'; } function showLoader() { if(hideLoader) return loadingWrapper.style.display = 'flex'; canvas.style.display = 'none'; } function showPaginator() { if(hidePaginator) return paginator.style.display = 'block'; } function queueRenderPage(num) { if (pageRendering) { pageNumPending = num; } else { renderPage(num); } } function onPrevPage() { if (pageNum = pdfDoc.numPages) { return; } pageNum++; queueRenderPage(pageNum); } document.getElementById('next').addEventListener('click', onNextPage); pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) { pdfDoc = pdfDoc_; var numPages = pdfDoc.numPages; document.getElementById('page_count').textContent = numPages; if(pageNum  numPages) { pageNum = numPages } renderPage(pageNum); }); }    PS ：如何实现 ？ 使用 hugo-embed-pdf-shortcode ，按照它写的来，但注意以下内容：\n url 路径一定搞清楚，比如我放在markdown文件旁边，url=\u0026quot;post/实习报告/实习报告.pdf\u0026quot; 把shortcode文件 embed-pdf.html 的第一行改成 \u0026lt;script src= '/js/pdf-js/build/pdf.js'\u0026gt;\u0026lt;/script\u0026gt;   ","date":"2022-03-11T11:52:08+08:00","permalink":"https://deathsprout.github.io/p/%E6%97%A7%E7%89%A9-%E5%A4%A7%E4%B8%89%E5%AE%9E%E4%B9%A0%E6%8A%A5%E5%91%8A-pdf%E5%B5%8C%E5%85%A5/","title":"旧物 | 大三实习报告 | PDF嵌入"},{"content":"大脑打印计划 🧠 主要参考 |  How I Converted my Brain fMRI to a 3D Model\n大脑数据来源？ 这需要你大脑的核磁共振扫描数据，推荐是关注周边高校和研究机构，有没有招被试的核磁实验，和实验者🥼沟通沟通，比如说希望试验结束能获得自己大脑的核磁数据。医院的核磁扫描是否能要到数据，目前我不了解。\n如果对面做了后续处理，可以试着要T1像的nii.gz格式数据，就可以跳过后面数据格式转换，如果要到的是原始数据，虽说可以通过dicombrowser 查看数据信息，但还是推荐询问实验者哪些是T1像。\n我的数据是3T场强的核磁扫出来了的，场强越大越精细，对3D打印模型精度影响不大，主要是可以通过建模软件对模型进行细分和平滑。\n数据转换 | 学习 FreeSurfer 和熟悉 fMRI 数据格式 先是在子系统里，用cd /mnt/e切换到E盘，mkdir freesurfer,cd进去下载和安装软件。可在此找符合电脑版本的安装包\nwget https://surfer.nmr.mgh.harvard.edu/pub/dist/freesurfer/7.2.0/freesurfer-linux-ubuntu18_amd64-7.2.0.tar.gz tar -zxv -f freesurfer-linux-ubuntu18_amd64-7.2.0.tar.gz #解压缩 需要在FreeSurfer Download and Registration中先注册，为的是获得license.txt，可能会显示FreeSurfer Registration Error reCAPTCHA not checked ,谷歌的人机验证reCAPTCHA无法正常使用，可以按照浏览器插件gooreplacer可以解决reCaptcha验证码无法加载的问题的方法解决（edge也有该插件），将license.txt放到./freesurfer。\nsudo chmod -R 777 ./freesurfer #改变freesurfer的权限 vi ~/.bashrc # 在~/.bashrc文件中添加以下两行 export FREESURFER_HOME= 你滴路径 source $FREESURFER_HOME/SetUpFreeSurfer.sh source ~/.bashrc sudo apt-get install tcsh #安装tcsh，类似与bash，不安装运行的时候会报错，相关资料是真滴少 recol-all --help #最后查看是否安装成功 关于数据 ： 我获得的数据的格式全部为IMA格式，.ima是西门子ct设备生成的医学图像文件格式之一，和.dicm文件格式一样（是二维的多张图像），数据是原始的下机数据。参考中他直接上手的数据是.nii.gz格式，是二进制压缩的NIFTI文件（三维图像），需要进行处理才能得到。\nsudo apt install dcm2niix # 装一个转换用的包 sudo apt-get install pigz # 按一个加速的支持 dcm2niix -o my_brain -f my_brain -z y brain_data/ # 输出文件夹 | 输出文件名,格式.nii.gz | 输入数据文件夹 先试试 gif_your_nifti 看看生成的gif结果如何，也是确保自己选的是否是T1的结构像。\ngit clone https://github.com/miykael/gif_your_nifti cd gif_your_nifti pip3 install -r requirements.txt #安装所需的py包，可以自己看一眼有没有没装的 python3 setup.py install #好习惯是激活个虚拟环境 # 转换，gif文件在输入文件的文件夹里生成， 可以看示例中的gif 调--cmap gif_your_nifti ../my_brain/my_brain.nii.gz --mode pseudocolor --cmap viridis   为了能扔图床上引用，进行了一定程度的压缩，还是能看出来就是要用的。\n剥头皮，获得 stl 导入blender   之后是使用freesurfer，进行漫长的剥头皮。（推荐过夜跑，我跑到快凌晨5点才弄完）\nmkdir -p subjects export SUBJECTS_DIR=$(pwd)/subjects recon-all -i 你滴.nii.gz -s 输出文件夹名字 -all   进入 surf，输入以下命令转化为stl格式，用blender打开。\nmris_convert lh.pial lh.pial.stl mris_convert rh.pial rh.pial.stl   左半脑简单的添加了材质  \nPart 1 | 联系3D打印农场 某群友推荐 wenext ，材料是树脂的话，价格0.3元/g，淘宝联系客服后加微信发了stl模型文件（记得将两半脑合并为一个stl导出，缩放选择0.001即是1：1大小 || 补：blender读取stl单位是m，那边的工业软件读取stl文件的单位是mm，所以不用放缩，具体和做3D打印的问问）\n我的1:1大脑参数如下  有亿点贵，长宽高放缩了0.7倍，加上邮费329 。（感觉也可以试着弄1:1但是中空，节省小钱钱）\nPart 2 | 玩耍  \n展示     \n✧*｡٩(ˊᗜˋ*)و✧*｡ 希望能在评论区看到大伙的大脑模型 🎉\n       ","date":"2022-02-08T16:34:04+08:00","image":"https://s2.loli.net/2022/02/08/XPDiofgdkRChBrz.png","permalink":"https://deathsprout.github.io/p/%E5%A6%82%E4%BD%953d%E6%89%93%E5%8D%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E8%84%91/","title":"如何3D打印自己的大脑"},{"content":"VGGFace是牛津大学视觉组于2015年发表，VGGNet也是他们提出的，是基于VGGNet的人脸识别模型。\n 文献 官网  为什么不能在pytorch上丝滑使用vggface 首先，vggface是基于vgg16架构的，pytorch本身也提供了vgg16等预训练模型（categories是imagenet_classes），见VGG-NETS。\n但是pytorch没有针对vggface数据集训练的vggface的预训练模型，你可以在官网的下载处看到提供的如下几种格式：\n vgg_face_matconvnet.tar.gz: Face detection and VGG Face descriptor source code and models (MatConvNet) vgg_face_torch.tar.gz: VGG Face descriptor source code and models (Torch) vgg_face_caffe.tar.gz: VGG Face descriptor source code and models (Caffe)  也许你会想，这不是有Torch格式的预训练模型吗？ 如果进行尝试，会发现是不行的。\n困难的真正原因是，之前的torch是使用lua语言，之后在2017年根据python重构了代码变成pytorch，而vgg-face的作者提供的是torch模型，而不是pytorch的模型。VGGface2是支持的（但vggface2数据集已经寄了），还是因为vggface有些年份了。\n实践 所以问题，出现了 ： pytorch 如何获得预训练模型 过去有\nfrom torch.utils.serialization import load_lua x = load_lua(\u0026#39;x.t7\u0026#39;) 但pytorch在1.0之后删除了torch.utils.serialization，目前可以通过torchfile.load读取，但会报错：\nTypeError: unhashable type: \u0026#39;numpy.ndarray\u0026#39; As of PyTorch 1.0 torch.utils.serialization is completely removed. Hence no one can import models from Lua Torch into PyTorch anymore. Instead, I would suggest installing PyTorch 0.4.1 through pip in a conda environment (so that you can remove it after this) and use this repo to convert your Lua Torch model to PyTorch model, not just the torch.nn.legacy model that you cannot use for training. Then use PyTorch 1.xx to do whatever with it. You can also train your converted Lua Torch models in PyTorch this way :) 来源\n但尝试失败，包括之后尝试的三个github的repo：PyVGGFace 、convert_torch_to_pytorch和 vgg-face.pytorch(issue中作者提到他仍能在linux中运行（今年8月))，希望转化成能用的形式，均因为相同的原因失败。\n发现网页Samuel Albanie,能下载网络框架的py文件，但不能下载含有权重信息的.pth文件（也可以不去下，底下完整代码里有这玩意）\n尝试使用GitHub上的caffemodel2pytorch(这玩意获得proto是通过request的，本地的prototxt文件读不进去)和Caffe2Pytorch(易用，但是0.4.1以上版本没有torch.legacy，而使用anaconda激活的虚拟环境中，pytorch0.4.1报错No module named “caffe”)\n依着上面提到的vgg-face.pytorch的issue的思路，使用自己电脑的Ubuntu子系统使用PyVGGFace，成功得到权重文件vggface.pth\n 链接: https://pan.baidu.com/s/1J8MbHZufFP2IRxUocomUxw?pwd=1wt6 提取码: 1wt6\n 如何将权重载入到模型框架里？ 如果直接使用torch.load导入：\nmodel = torch.load(r\u0026#34;D:\\Dataset\\models\\vggface.pth\u0026#34;) model.eval() AttributeError: \u0026#39;collections.OrderedDict\u0026#39; object has no attribute \u0026#39;eval\u0026#39; 原因是这仅是字典形式的权重数据，没有模型的实体。想用来自Samuel Albanie的模型框架文件获得转化后的权重数据，报错如下，可以看出是字典的键值不匹配。\nRuntimeError: Error(s) in loading state_dict for Vgg_face_dag: Missing key(s) in state_dict: \u0026#34;conv_1_1.weight\u0026#34;, \u0026#34;conv_1_1.bias\u0026#34;, \u0026#34;conv1_2.weight\u0026#34;, \u0026#34;conv1_2.bias\u0026#34;, \u0026#34;conv2_1.weight\u0026#34;, \u0026#34;conv2_1.bias\u0026#34;, \u0026#34;conv2_2.weight\u0026#34;, \u0026#34;conv2_2.bias\u0026#34;, \u0026#34;conv3_1.weight\u0026#34;, \u0026#34;conv3_1.bias\u0026#34;, \u0026#34;conv3_2.weight\u0026#34;, \u0026#34;conv3_2.bias\u0026#34;, \u0026#34;conv3_3.weight\u0026#34;, \u0026#34;conv3_3.bias\u0026#34;, \u0026#34;conv4_1.weight\u0026#34;, \u0026#34;conv4_1.bias\u0026#34;, \u0026#34;conv4_2.weight\u0026#34;, \u0026#34;conv4_2.bias\u0026#34;, \u0026#34;conv4_3.weight\u0026#34;, \u0026#34;conv4_3.bias\u0026#34;, \u0026#34;conv5_1.weight\u0026#34;, \u0026#34;conv5_1.bias\u0026#34;, \u0026#34;conv5_2.weight\u0026#34;, \u0026#34;conv5_2.bias\u0026#34;, \u0026#34;conv5_3.weight\u0026#34;, \u0026#34;conv5_3.bias\u0026#34;, \u0026#34;fc6.weight\u0026#34;, \u0026#34;fc6.bias\u0026#34;, \u0026#34;fc7.weight\u0026#34;, \u0026#34;fc7.bias\u0026#34;, \u0026#34;fc8.weight\u0026#34;, \u0026#34;fc8.bias\u0026#34;. Unexpected key(s) in state_dict: \u0026#34;features.conv_1_1.weight\u0026#34;, \u0026#34;features.conv_1_1.bias\u0026#34;, \u0026#34;features.conv_1_2.weight\u0026#34;, \u0026#34;features.conv_1_2.bias\u0026#34;, \u0026#34;features.conv_2_1.weight\u0026#34;, \u0026#34;features.conv_2_1.bias\u0026#34;, \u0026#34;features.conv_2_2.weight\u0026#34;, \u0026#34;features.conv_2_2.bias\u0026#34;, \u0026#34;features.conv_3_1.weight\u0026#34;, \u0026#34;features.conv_3_1.bias\u0026#34;, \u0026#34;features.conv_3_2.weight\u0026#34;, \u0026#34;features.conv_3_2.bias\u0026#34;, \u0026#34;features.conv_3_3.weight\u0026#34;, \u0026#34;features.conv_3_3.bias\u0026#34;, \u0026#34;features.conv_4_1.weight\u0026#34;, \u0026#34;features.conv_4_1.bias\u0026#34;, \u0026#34;features.conv_4_2.weight\u0026#34;, \u0026#34;features.conv_4_2.bias\u0026#34;, \u0026#34;features.conv_4_3.weight\u0026#34;, \u0026#34;features.conv_4_3.bias\u0026#34;, \u0026#34;features.conv_5_1.weight\u0026#34;, \u0026#34;features.conv_5_1.bias\u0026#34;, \u0026#34;features.conv_5_2.weight\u0026#34;, \u0026#34;features.conv_5_2.bias\u0026#34;, \u0026#34;features.conv_5_3.weight\u0026#34;, \u0026#34;features.conv_5_3.bias\u0026#34;, \u0026#34;fc.fc6.weight\u0026#34;, \u0026#34;fc.fc6.bias\u0026#34;, \u0026#34;fc.fc7.weight\u0026#34;, \u0026#34;fc.fc7.bias\u0026#34;, \u0026#34;fc.fc8.weight\u0026#34;, \u0026#34;fc.fc8.bias\u0026#34;. 实测不能通过在定义层时在conv前添加features.xxx解决\nself.add_module(\u0026#34;features.conv_1_1\u0026#34;,nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))) KeyError: \u0026#39;module name can\\\u0026#39;t contain \u0026#34;.\u0026#34;, got: features.conv_1_1\u0026#39; 写函数,得到新字典后成功获得完整模型实例\ndef transform_key(model): #列表来自报错内容，可复制进来更改，创建一个新字典，将key 从post_names -\u0026gt; names names = [\u0026#34;conv_1_1.weight\u0026#34;, \u0026#34;conv_1_1.bias\u0026#34;, \u0026#34;conv1_2.weight\u0026#34;, \u0026#34;conv1_2.bias\u0026#34;, \u0026#34;conv2_1.weight\u0026#34;, \u0026#34;conv2_1.bias\u0026#34;, \u0026#34;conv2_2.weight\u0026#34;, \u0026#34;conv2_2.bias\u0026#34;, \u0026#34;conv3_1.weight\u0026#34;, \u0026#34;conv3_1.bias\u0026#34;, \u0026#34;conv3_2.weight\u0026#34;, \u0026#34;conv3_2.bias\u0026#34;, \u0026#34;conv3_3.weight\u0026#34;, \u0026#34;conv3_3.bias\u0026#34;, \u0026#34;conv4_1.weight\u0026#34;, \u0026#34;conv4_1.bias\u0026#34;, \u0026#34;conv4_2.weight\u0026#34;, \u0026#34;conv4_2.bias\u0026#34;, \u0026#34;conv4_3.weight\u0026#34;, \u0026#34;conv4_3.bias\u0026#34;, \u0026#34;conv5_1.weight\u0026#34;, \u0026#34;conv5_1.bias\u0026#34;, \u0026#34;conv5_2.weight\u0026#34;, \u0026#34;conv5_2.bias\u0026#34;, \u0026#34;conv5_3.weight\u0026#34;, \u0026#34;conv5_3.bias\u0026#34;, \u0026#34;fc6.weight\u0026#34;, \u0026#34;fc6.bias\u0026#34;, \u0026#34;fc7.weight\u0026#34;, \u0026#34;fc7.bias\u0026#34;, \u0026#34;fc8.weight\u0026#34;, \u0026#34;fc8.bias\u0026#34;] post_names = [\u0026#34;features.conv_1_1.weight\u0026#34;, \u0026#34;features.conv_1_1.bias\u0026#34;, \u0026#34;features.conv_1_2.weight\u0026#34;, \u0026#34;features.conv_1_2.bias\u0026#34;, \u0026#34;features.conv_2_1.weight\u0026#34;, \u0026#34;features.conv_2_1.bias\u0026#34;, \u0026#34;features.conv_2_2.weight\u0026#34;, \u0026#34;features.conv_2_2.bias\u0026#34;, \u0026#34;features.conv_3_1.weight\u0026#34;, \u0026#34;features.conv_3_1.bias\u0026#34;, \u0026#34;features.conv_3_2.weight\u0026#34;, \u0026#34;features.conv_3_2.bias\u0026#34;, \u0026#34;features.conv_3_3.weight\u0026#34;, \u0026#34;features.conv_3_3.bias\u0026#34;, \u0026#34;features.conv_4_1.weight\u0026#34;, \u0026#34;features.conv_4_1.bias\u0026#34;, \u0026#34;features.conv_4_2.weight\u0026#34;, \u0026#34;features.conv_4_2.bias\u0026#34;, \u0026#34;features.conv_4_3.weight\u0026#34;, \u0026#34;features.conv_4_3.bias\u0026#34;, \u0026#34;features.conv_5_1.weight\u0026#34;, \u0026#34;features.conv_5_1.bias\u0026#34;, \u0026#34;features.conv_5_2.weight\u0026#34;, \u0026#34;features.conv_5_2.bias\u0026#34;, \u0026#34;features.conv_5_3.weight\u0026#34;, \u0026#34;features.conv_5_3.bias\u0026#34;, \u0026#34;fc.fc6.weight\u0026#34;, \u0026#34;fc.fc6.bias\u0026#34;, \u0026#34;fc.fc7.weight\u0026#34;, \u0026#34;fc.fc7.bias\u0026#34;, \u0026#34;fc.fc8.weight\u0026#34;, \u0026#34;fc.fc8.bias\u0026#34;] from collections import OrderedDict new_state_dict = OrderedDict() for i in range(0,len(model)): name = names[i] new_state_dict[name] = model[post_names[i]] return new_state_dict 完整代码 import torch import torch.nn as nn class Vgg_face_dag(nn.Module): def __init__(self): super(Vgg_face_dag, self).__init__() self.meta = {\u0026#39;mean\u0026#39;: [129.186279296875, 104.76238250732422, 93.59396362304688], \u0026#39;std\u0026#39;: [1, 1, 1], \u0026#39;imageSize\u0026#39;: [224, 224, 3]} self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu1_1 = nn.ReLU(inplace=True) self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu1_2 = nn.ReLU(inplace=True) self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False) self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu2_1 = nn.ReLU(inplace=True) self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu2_2 = nn.ReLU(inplace=True) self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False) self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu3_1 = nn.ReLU(inplace=True) self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu3_2 = nn.ReLU(inplace=True) self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu3_3 = nn.ReLU(inplace=True) self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False) self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu4_1 = nn.ReLU(inplace=True) self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu4_2 = nn.ReLU(inplace=True) self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu4_3 = nn.ReLU(inplace=True) self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False) self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu5_1 = nn.ReLU(inplace=True) self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu5_2 = nn.ReLU(inplace=True) self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1)) self.relu5_3 = nn.ReLU(inplace=True) self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False) self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True) self.relu6 = nn.ReLU(inplace=True) self.dropout6 = nn.Dropout(p=0.5) self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True) self.relu7 = nn.ReLU(inplace=True) self.dropout7 = nn.Dropout(p=0.5) self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True) def forward(self, x0): x1 = self.conv1_1(x0) x2 = self.relu1_1(x1) x3 = self.conv1_2(x2) x4 = self.relu1_2(x3) x5 = self.pool1(x4) x6 = self.conv2_1(x5) x7 = self.relu2_1(x6) x8 = self.conv2_2(x7) x9 = self.relu2_2(x8) x10 = self.pool2(x9) x11 = self.conv3_1(x10) x12 = self.relu3_1(x11) x13 = self.conv3_2(x12) x14 = self.relu3_2(x13) x15 = self.conv3_3(x14) x16 = self.relu3_3(x15) x17 = self.pool3(x16) x18 = self.conv4_1(x17) x19 = self.relu4_1(x18) x20 = self.conv4_2(x19) x21 = self.relu4_2(x20) x22 = self.conv4_3(x21) x23 = self.relu4_3(x22) x24 = self.pool4(x23) x25 = self.conv5_1(x24) x26 = self.relu5_1(x25) x27 = self.conv5_2(x26) x28 = self.relu5_2(x27) x29 = self.conv5_3(x28) x30 = self.relu5_3(x29) x31_preflatten = self.pool5(x30) x31 = x31_preflatten.view(x31_preflatten.size(0), -1) x32 = self.fc6(x31) x33 = self.relu6(x32) x34 = self.dropout6(x33) x35 = self.fc7(x34) x36 = self.relu7(x35) x37 = self.dropout7(x36) x38 = self.fc8(x37) return x38 def transform_key(state_dict): #列表来自报错内容，可复制进来更改，创建一个新字典，将key 从post_names -\u0026gt; names names = [\u0026#34;conv1_1.weight\u0026#34;,\u0026#34;conv1_1.bias\u0026#34;, \u0026#34;conv1_2.weight\u0026#34;, \u0026#34;conv1_2.bias\u0026#34;, \u0026#34;conv2_1.weight\u0026#34;, \u0026#34;conv2_1.bias\u0026#34;, \u0026#34;conv2_2.weight\u0026#34;, \u0026#34;conv2_2.bias\u0026#34;, \u0026#34;conv3_1.weight\u0026#34;, \u0026#34;conv3_1.bias\u0026#34;, \u0026#34;conv3_2.weight\u0026#34;, \u0026#34;conv3_2.bias\u0026#34;, \u0026#34;conv3_3.weight\u0026#34;, \u0026#34;conv3_3.bias\u0026#34;, \u0026#34;conv4_1.weight\u0026#34;, \u0026#34;conv4_1.bias\u0026#34;, \u0026#34;conv4_2.weight\u0026#34;, \u0026#34;conv4_2.bias\u0026#34;, \u0026#34;conv4_3.weight\u0026#34;, \u0026#34;conv4_3.bias\u0026#34;, \u0026#34;conv5_1.weight\u0026#34;, \u0026#34;conv5_1.bias\u0026#34;, \u0026#34;conv5_2.weight\u0026#34;, \u0026#34;conv5_2.bias\u0026#34;, \u0026#34;conv5_3.weight\u0026#34;, \u0026#34;conv5_3.bias\u0026#34;, \u0026#34;fc6.weight\u0026#34;, \u0026#34;fc6.bias\u0026#34;, \u0026#34;fc7.weight\u0026#34;, \u0026#34;fc7.bias\u0026#34;, \u0026#34;fc8.weight\u0026#34;, \u0026#34;fc8.bias\u0026#34;] post_names = [\u0026#34;features.conv_1_1.weight\u0026#34;, \u0026#34;features.conv_1_1.bias\u0026#34;, \u0026#34;features.conv_1_2.weight\u0026#34;, \u0026#34;features.conv_1_2.bias\u0026#34;, \u0026#34;features.conv_2_1.weight\u0026#34;, \u0026#34;features.conv_2_1.bias\u0026#34;, \u0026#34;features.conv_2_2.weight\u0026#34;, \u0026#34;features.conv_2_2.bias\u0026#34;, \u0026#34;features.conv_3_1.weight\u0026#34;, \u0026#34;features.conv_3_1.bias\u0026#34;, \u0026#34;features.conv_3_2.weight\u0026#34;, \u0026#34;features.conv_3_2.bias\u0026#34;, \u0026#34;features.conv_3_3.weight\u0026#34;, \u0026#34;features.conv_3_3.bias\u0026#34;, \u0026#34;features.conv_4_1.weight\u0026#34;, \u0026#34;features.conv_4_1.bias\u0026#34;, \u0026#34;features.conv_4_2.weight\u0026#34;, \u0026#34;features.conv_4_2.bias\u0026#34;, \u0026#34;features.conv_4_3.weight\u0026#34;, \u0026#34;features.conv_4_3.bias\u0026#34;, \u0026#34;features.conv_5_1.weight\u0026#34;, \u0026#34;features.conv_5_1.bias\u0026#34;, \u0026#34;features.conv_5_2.weight\u0026#34;, \u0026#34;features.conv_5_2.bias\u0026#34;, \u0026#34;features.conv_5_3.weight\u0026#34;, \u0026#34;features.conv_5_3.bias\u0026#34;, \u0026#34;fc.fc6.weight\u0026#34;, \u0026#34;fc.fc6.bias\u0026#34;, \u0026#34;fc.fc7.weight\u0026#34;, \u0026#34;fc.fc7.bias\u0026#34;, \u0026#34;fc.fc8.weight\u0026#34;, \u0026#34;fc.fc8.bias\u0026#34;] from collections import OrderedDict new_state_dict = OrderedDict() for i in range(0,len(state_dict)): name = names[i] new_state_dict[name] = state_dict[post_names[i]] return new_state_dict def vgg_face(weights_path=None, **kwargs): # 实例化模型，weights_path=None 表示随机初始化权重 \u0026#34;\u0026#34;\u0026#34; load imported model instance Args: weights_path (str): If set, loads model weights from the given path \u0026#34;\u0026#34;\u0026#34; model = Vgg_face_dag() if weights_path: state_dict = torch.load(weights_path) new_state_dict = transform_key(state_dict) model.load_state_dict(new_state_dict) model.eval() return model vggface = vgg_face(weights_path= r\u0026#34;D:\\Dataset\\models\\vggface.pth\u0026#34;) # 实例化模型， weights_path 是权重文件路径 ","date":"2021-12-07T17:53:33+08:00","image":"https://s2.loli.net/2021/12/07/e1lHXDzs5pMjirJ.png","permalink":"https://deathsprout.github.io/p/vggface-pytorch-%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/","title":"Vggface Pytorch 完全指南"},{"content":" \n\nTODO    从 HugoTex theme 更换到 stack theme\n  更改 点击home会在新页面弹出 ——\u0026gt; 本页刷新\n  更改文章的归档类别，精简 categories\n  Google 百度等收录网页\n  更换挂载服务商\n垃圾gitee（ gitee page 显示index.json可能含有违规内容，删去，导致blog的搜索功能失效 ）\n  添加字数统计\n 单文章字数统计 footer 总字数和文章数统计    添加音乐\n  添加更多Markdown支持\n  shortcode  文字居中\n  支持表情 支持github格式的 !!! note 和 !!! danger github 风格的 chackbox ，明显的蓝色 √ 将KaTex支持升级 支持使用 Font awesome     页面美化\n  折叠式侧边栏\n  字体\n 更换字体 调整字体大小    颜色\n 页面配色  底色 滑块颜色   link 颜色 赛博朋克块阴影 (不要了)    头像与名称居中显示\n  调整页面比例，比较喜欢缩放到80%~90%\n  增加网站icon\n  使用物色到的icon\n 字数统计、显示时间的icon    修改 card 圆角 10 $\\Rightarrow$ 7\n  圆角 tag --tag-border-radius: 4px; $\\Rightarrow$ 20\n    社交\n 编写自己的 about 添加评论功能 完善其他跳转信息，如脸书可是我没有、知乎等账号链接 添加友链    记录  HugoTex theme 更换到 stack theme 添加字数统计 单文章字数统计 : 借鉴小球飞鱼，改layouts\\partials\\article\\components\\details.html\n{{ if .Site.Params.article.readingTime }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;文本\u0026#34; }} \u0026lt;time class=\u0026#34;article-words\u0026#34;\u0026gt; {{ .WordCount }}字 \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; 并在config.yaml设置hasCJKLanguage: true\n总字数和文章数统计 : Hugo 总文章数和总字数\n页面美化 字体 更换字体 看对应部分，简单的复制粘贴 (不过之后想换成方正新书宋或者冬青黑，英文标题部分用贝连体)\n调整字体大小 位置 assets\\scss\\variables.scss 中\n--article-font-size: 1.7rem; $\\Rightarrow$ 1.9\n颜色 页面配色 底色 :\n light ☞ 改了一圈 仅将背景色弄浅了一点 dark ☞ --body-background: #1e202b; 颜色是从中国传统色漆黑调浅  滑块颜色:\n light --scrollbar-thumb: hsla(170, 94%, 55%, 0.575); dark --scrollbar-thumb: #c91f37; 赫赤色  link 颜色 从灰色改为 --link-background-color: 1, 255, 179;\n赛博朋克块阴影 --shadow-l1: 1.4px -1.4px 0px rgba(67, 255, 230, 0.719), 0px 3px 10px rgba(0, 0, 0, 0.04), 0px 1.4px 1px rgba(253, 25, 113, 0.397); 更多markdown支持              在文档末添加如下，就可以使用 Font awesome 5 的非Pro icon , 使用方式也很便捷，直接在Font awesome 里面找，点复制粘贴过来就可以了。\n\u0026lt;head\u0026gt; \u0026lt;script defer src=\u0026#34;https://use.fontawesome.com/releases/v5.11.0/js/all.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://use.fontawesome.com/releases/v5.11.0/js/v4-shims.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://use.fontawesome.com/releases/v5.11.0/css/all.css\u0026#34;\u0026gt; 不花钱  没必要使用Font awesome 6 ，不如去阿里弄得iconfont 去淘 icon，下面是后来的配置，两方面都能使用。\n\u0026lt;head\u0026gt; \u0026lt;script src=\u0026#34;https://kit.fontawesome.com/cbd1a514ae.js\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; #Font awesome 的kit，注册后创建一个 \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;http://at.alicdn.com/t/font_3178786_wcuwkyqh11.css\u0026#34;\u0026gt; #iconfont 账号的项目，注册后创建一个 \u0026lt;/head\u0026gt; 但是有一些问题，比如\n   不能很好的显示彩色icon (包括font awesome)，后面的两个icon是下面彩色icon现在的显示情况。              每次往iconfont项目库里添加新icon，都要更新项目库，把项目库新地址复制过来，有点麻烦。    # 在head里添加 \u0026lt;script src=\u0026#34;https://at.alicdn.com/t/font_3178786_idyehkf29xd.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; ---------------- # 上面两个彩svg的引用代码 \u0026lt;div\u0026gt; \u0026lt;svg class=\u0026#34;icon\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;use xlink:href=\u0026#34;#icon-xinxi\u0026#34;\u0026gt;\u0026lt;/use\u0026gt; \u0026lt;/svg\u0026gt; \u0026lt;svg class=\u0026#34;icon\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;use xlink:href=\u0026#34;#icon-10\u0026#34;\u0026gt;\u0026lt;/use\u0026gt; \u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; ---------------- \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; .icon { width: 1em; height: 1em; # 可以通过这种方式控制icon大小 vertical-align: -0.15em; fill: currentColor; overflow: hidden; } \u0026lt;/style\u0026gt;   如果是做前端，还稍微可以接受。对于写markdown笔记，这么做实在是太麻烦了点(强迫症震怒)，看到有使用 vue  封装简化操作的（有一点探索过头了，活该是有追求的前端干的事）。 但无论怎么说，在markdown里用这么多HTML语句和初衷有些本末倒置。 \n Shortcode  ⭕ 能用 | ❌ 一番尝试后失败\n 一开始想将Feelit上的shortcode抄过来，具体是admonition⭕ music typeit，把\\layouts\\shortcodes\\复制进来没法正常用，自定义 Hugo Shortcodes 简码写的很详细，搬过来align⭕ quote⭕,\n水水水\n This is a tip  一个 技巧 横幅   Typeit❌\n new TypeIt(\"#simpleUsage\", { strings: \"This is a simple string.\", speed: 50, waitUntilVisible: true, }).go();  评论功能 吆西，直接参照waline文档，先注册个 LeanCloud , 注意是国际版，然后再GitHub登录下Vercel ，是从 快速上手\u0026ndash;Vercel 部署 点击过去（球球网你让我登录！）\nwaline文档 快速上手中 除了 HTML引入(客户端) 以外的照着做就好，之后在config.yaml 里改前端配置\ncomments:enabled:trueprovider:walinewaline:serverURL://你通过vercel获得的waline服务器地址lang:\u0026#39;zh-CN\u0026#39;visitor:trueavatar:emoji:- https://cdn.jsdelivr.net/gh/walinejs/emojis/weibo- https://cdn.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs- https://cdn.jsdelivr.net/gh/norevi/blob-emoji-for-waline@2.0/blobs-gifrequiredMeta:- name- email- urlplaceholder:locale:admin:Admin更多的（表情包、修改黑暗模式和表情包大小）移步到参考链接，基本上是复制粘贴\n参考 小球飞鱼 | Hugo | 看中 Stack 主题的归档功能，搬家并做修改\nCSS（层叠样式表）\n中国传统颜色手册\n小球飞鱼 | Hugo | 为 Blog 增加评论区\nwaline文档\nNo.revi | Hugo｜引入外部字体\nNo.revi | Waline｜添加自定义表情\n在 Hugo 主题 Stack 中轻松配置 Waline 评论系统\n花裤衩coder | 手摸手，带你优雅的使用 icon\nHugo嵌入音乐播放器\nAPlayer \u0026amp; MetingJS 音乐播放器使用指南\n    ","date":"2021-12-06T22:31:49+08:00","image":"https://s2.loli.net/2021/12/07/1rVtuJzys7Foecp.jpg","permalink":"https://deathsprout.github.io/p/blog%E6%9B%B4%E6%94%B9%E6%97%A5%E5%BF%97/","title":"Blog更改日志"},{"content":"Autoencoder 自编码器是一种神经网络，其设计目的是在压缩数据的同时，以无监督的方式学习恒等函数来重构原始输入，从而发现一种更有效的压缩表示。\n Encoder network $g_\\phi(.)$：把原始的高维输入转换成潜在的低维编码，输入大小大于输出大小。 Decoder network $f_\\theta(.)$：从编码中复原数据. bottleneck layer 是 $z=g_\\phi(x)$  花书以 $h=f(x)$ 表示编码器的输出\n  重建数据 ：$x'=f_\\theta(g_\\phi(x))$   \n维度压缩就像PCA或MF，而从编码再重建数据，好的中间表示不仅可以捕获潜在变量，也有利于整个的解压缩过程，属于是对自编码器进行了显式优化。\n参数$(\\theta,\\phi)$一起学习，令$x\\approx f_\\theta(g_\\phi(x))$,有很多方法可以量化这两个向量之间的差异，比如激活函数为sigmoid时的交叉熵，或者简单的MSE损失\n$$L_{AE}(θ,ϕ)=\\frac{1}{n}∑_{i=1}^{n}(x^{(i)}−f_θ(g_ϕ(x^{(i)})))^2$$\n 在花书中，缩写为 $L(x,g(f(x)))$\n Denoising Autoencoder (DAE) 当网络参数大于数据点数的时候，面临着过拟合的风险，为避免过拟合和提高鲁棒性，输入被随机方式加入噪声或掩盖输入向量的某些值而部分损坏，记为\n$$\\tilde{x}^{(i)} \\sim \\mathcal{M_D}(\\tilde{x}^{(i)}|x^{(i)})$$\n$\\mathcal{M_D}$定义了从真实的数据样本到噪声或损坏的数据样本的映射\n最小化 $L(x,g(f(\\tilde{x})))$，重建的数据是无噪声的  \nSparse Autoencoder 迫使模型在同一时间只有少量的隐藏单元被激活，一个隐藏层的神经元应该在大部分时间被灭活。 一隐藏层神经元被激活的比例是一个参数 $\\hat{\\rho}$ ,期望应该是一个很小的数 $\\rho$ (叫做稀疏参数),通常 $\\rho=0.05$\n这个约束是通过在损失函数中添加一个惩罚项实现的，KL散度测量了平均值为$\\rho$和$\\hat{\\rho}$两个伯努利分布（Bernoulli distributions）之间的差别。用超参数$\\beta$来控制对稀疏损失的惩罚程度。\n \n Notation： 关于自编码器符号，花书和From Autoencoder to Beta-VAE是有区别的，具体是Encoder、Decoder（相反）和中间数据表示符号（z 、h）不同.\n Structured probabilistic model 结构化概率模型使用图来描述概率分布中随机变量之间的直接相互作用,从而描述一个概率分布，这些模型也通常被称为图模型（graphical model） 。\n忽略间接相互作用，能大大减少模型的参数个数，更小的模型大大减少了在模型存储、模型推断以及在模型中采样时的计算开销。\n图描述模型结构 有向模型 有向图模型（directed graphical model），也被称为信念网络（belief network）或者贝叶斯网络（Bayesian network）。\n变量$x$的有向概率模型是通过有向无环图$\\mathcal{G}$和一系列局部条件概率分布来定义的。其中，$P_{a\\mathcal{G}}(x_i)$表示节点$x_i$的所有父节点。 $$p(x)=\\prod_ip(x_i|P_{a\\mathcal{G}}(x_i))$$\n无向模型 无向模型（undirected model），也被称为马尔可夫随机场（Markov random field，MRF）或者马尔可夫网络。\n并不是所有情况的相互作用都有一个明确的方向关系，当相互作用没有本质性的指向或者是明确的双向关系作用时，用无向模型更合适。\n对于图中的每一个团 $\\mathcal{C}$ (图中的团是图中节点的一个子集，其中的点是全连接的), 一个因子 $\\phi(\\mathcal{C})$ (也被称为团势能（clique potential）)，衡量了团中每个变量每一种可能的联合状态所对应的密切程度。它们一起定义了未归一化概率函数： $$\\tilde{p}(x)=\\prod_{\\mathcal{C\\in G}}\\phi(\\mathcal{C})$$\n配分函数 为了保证概率之和或积分为1，需要使用对应的归一化的概率分布\n$$p(x)=\\frac{1}{Z}\\tilde{p}(x)$$\n$$Z = \\int \\tilde{p}(x) dx$$\n当函数$\\phi$固定时，$Z$就是个常数，如果函数$\\phi$带有参数，$Z$就是这些参数的一个函数，$Z$被称为配分函数。\n分离与d-分离 想知道在给定其他变量子集的值时，哪些变量子集彼此条件独立。\n无向模型中，识别图中的条件独立性是非常简单的，这时候图中隐含的条件独立性称为分离。 如果变量a和b的连接路径仅涉及未观察变量，那么这些变量不是分离的。如果之间没有路径，或者所有路径都包含可观测的变量，那么他们是分离的。认为仅涉及未观察到的变量的路径是“活跃”的，包括可观察变量的路径称为“非活跃”的。\n有向模型中，这些概念叫做d-分离。\nVAE: Variational Autoencoder 不想把输入映射到一个固定的向量，而是要把它映射到一个概率分布$p_\\theta$ 。输入$x$和latent encoding vector $z$ 之间的关系可以被下面的定义：\n 先验分布 $p_\\theta(z)$ 似然分布 $p_\\theta(x|z)$ 后验分布 $p_\\theta(z|x)$  为了生成一个像是真实数据的样本$x^i$，以下步骤：\n 从先验分布$p_{\\theta^*}(z)$中抽样$z^i$ 从条件概率分布$p_{\\theta^*}(x|z=z^i)$中生成值$x^i$  最佳参数 $\\theta^*$ 是使生成真实样本数据概率最大的参数。\n$$ \\theta^* = arg \\ max_\\theta\\prod^n_{i=1}p_\\theta(x^i)$$\n通常会变形为log形式，将乘法换成加法\n$$\\theta^* = arg \\ max_\\theta\\prod^n_{i=1}log \\ p_\\theta(x^i)$$\n$$p_\\theta(x^i)=\\int p_\\theta(x^i|z)p_\\theta(z)dz$$\n但是检查所有可能的$z$值，并将对应的x的概率积分的代价是很昂贵的。引入概率分布$q_\\phi(z|x)$去近似$p_\\theta(z|x)$，用输出$x$给定输入可能的$z$\n \n如上，现在的结构很像一个自编码器\n $p_\\theta(x|z)$ 定义了一个生成模型，类似于解码器，$p_\\theta(x|z)$ 也叫概率解码器 估计的函数 $q_\\phi(z|x)$ 是概率编码器  Loss function：ELBO 可以用KL散度去量化这两个分布之间的距离，$D_{KL}(X||Y)$ ,度量用分布Y表示X时丢失了多少信息。\n我们想要参数$\\phi$，使得$D_{KL}(q_\\phi(z|x)||p_\\theta(z|x))$最小\n为什么是 $D_{KL}(q_\\phi||p_\\theta)$ (reversed KL) 而不是 $D_{KL}(p_\\theta||q_\\phi)$ (forward KL)  [Bayesian Variational methods -- Eric Jang](https://blog.evjang.com/2016/08/variational-bayes.html) ![](https://i.loli.net/2021/11/24/N15t2gJQXbAmLRE.png)    \tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;  \n\tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;  \n这个是学习真实分布时想要最大化的：最大化log形式的产生真实数据的可能性( $log \\ p_\\theta(x)$ ),最小化真实后验分布和估计的后验分布的差异。\n损失函数的定义如下：\n\tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;  \n在变分贝叶斯方法中，这个损失函数被称为变分下界或证据下界。KL散度总是非负的，$-L_{VAE}$ 就是 $log \\ p_\\theta(x)$ 的下界。\n$$-L_{VAE}=log \\ p_\\theta(x) - D_{KL}(q_\\phi(z|x)||p_\\theta(z|x)) \\leq log \\ p_\\theta(x)$$\n最小化损失函数，就能最大化产生真实数据样本的概率下界\nReparameterization Trick （重新参数化技巧） 损失函数中调用了$z\\sim q_\\theta(z|x)$的生成样本，采样是一个随机过程，不能反向传播梯度，为了使之可以训练，引入了重新参数化技巧。 通常是将随机变量$z$转化为确定性变量 $z=\\mathcal{T}_\\theta(x,\\epsilon)$,其中$\\epsilon$是一个独立的随机变量。\n\tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;  \n\tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;  \n上图说明了重新参数化技巧如何使$z$采样过程可训练\n\tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;  \n$\\odot$ Hadamard product  Hadamard 积，只在两个相同维度的矩阵(A\\B)中定义，记作 $A\\odot B \\ or \\ A \\circ B$ 运算则是逐元素相乘\n\tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;   \n  $\\beta$-VAE 如果 inferred latent representation $z$ 只对单一生成因素敏感，对其他因素不敏感，叫这种表示为解纠缠（disentangled）或因子化的（factorized）。好处是可解释性好，而且易于迁移到其他任务。\n例如，一个模型在训练人脸照片时可能会捕捉肤色、头发颜色、头发长度、情绪、是否戴眼镜以及许多其他相对独立的因素。这样的解纠缠表示对于人脸图像的生成是非常有益的。\n$\\beta$-VAE是修改的VAE,特别强调发现解纠缠的潜在因素。\n\tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;  \nKKT条件下可以重写为有拉格朗日乘子$\\beta$的拉格朗日函数，上述只有一个不等式约束的优化问题等价于最大化方程$\\mathcal{F}(\\theta,\\phi,\\beta)$\n\tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;  \n\tloading=\u0026quot;lazy\u0026quot; \u0026gt; \u0026lt;/a\u0026gt;  \n$\\beta$作为超参数，如果$\\beta=1$，和VAE相同，更高的$\\beta$值强化了对latent bottleneck的约束，能增强解纠缠。（和正则化中的权重衰减类似）\n参考  花书第十四章 花书 第16章 深度学习中的结构化概率模型 From Autoencoder to Beta-VAE Variational autoencoders.           ","date":"2021-12-05T23:49:21+08:00","image":"https://s2.loli.net/2021/12/06/diyx52IXO7aWuPw.png","permalink":"https://deathsprout.github.io/p/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/","title":"变分自编码器"},{"content":"问题描述 在一次夜间放着跑程序时被母上拔掉了电源，应当是电脑启动了系统自动更新导致系统损坏，蓝屏折腾好久最终还是重装了系统。最近需要使用matlab上的工具包matconvnet，过程中先后遇到了如下两个问题。\n  问题一     mex -setup\n    错误使用 mex 未找到支持的编译器。您可以安装免费提供的 MinGW-w64 C/C++ 编译器；请参阅安装 MinGW-w64 编译器。有关更多选项，请访问 https://www.mathworks.com/support/compilers。\n  重装系统之前，c的编译器电脑中是有MinGW以及vscode的，观察环境变量所指的位置仍然有这些文件，（但注册表那边肯定是无了）尝试重新安装了一个MinGW并配置好环境变量，仍然无用。一开始是怀疑配置文件中编辑器的版本和地址与实际情况不符，也因为我的matlab是2018b，顺手删了更新下2020a，发现问题仍然存在。\n详细报错\n\u0026gt;\u0026gt; mex -setup -V 详细模式已开。 略 ... 正在查找编译器 \u0026#39;MinGW64 Compiler (C)\u0026#39;... ... 正在查找环境变量 \u0026#39;MW_MINGW64_LOC\u0026#39;...否。 找不到已安装的编译器 \u0026#39;MinGW64 Compiler (C)\u0026#39;。 略 可以看出matlab找MinGW编辑器是以MW_MINGW64_LOC为环境变量名的,之前我都是将它的环境变量放到path里面去,在重新创建一个名为MW_MINGW64_LOC的环境变量,值设为新路径之后(D:\\for_sci\\new_minGW\\mingw64)。\n\u0026gt;\u0026gt; setenv(\u0026#39;MW_MINGW64_LOC\u0026#39;,\u0026#39;D:\\for_sci\\new_minGW\\mingw64\u0026#39;) \u0026gt;\u0026gt; mex -setup MEX 配置为使用 \u0026#39;MinGW64 Compiler (C)\u0026#39; 以进行 C 语言编译。 要选择不同的语言，请从以下选项中选择一种命令: mex -setup C++ mex -setup FORTRAN MEX 配置为使用 \u0026#39;MinGW64 Compiler (C++)\u0026#39; 以进行 C++ 语言编译。 MEX 配置为使用 \u0026#39;MinGW64 Compiler (C++)\u0026#39; 以进行 C++ 语言编译。   问题二   \u0026gt;\u0026gt; vl_compilenn(\u0026#39;verbose\u0026#39;, 1) 警告: 以后的版本中将会取消对 toolboxdir(\u0026#39;distcomp\u0026#39;) 的支持。请改用 toolboxdir(\u0026#39;parallel\u0026#39;)。 \u0026gt; In toolboxdir (line 49) In vl_compilenn (line 367) 警告: CL.EXE not found in PATH. Trying to guess out of mex setup. \u0026gt; In vl_compilenn\u0026gt;check_clpath (line 650) In vl_compilenn (line 426) \u0026#39;cl.exe\u0026#39; 不是内部或外部命令，也不是可运行的程序 或批处理文件。 错误使用 vl_compilenn\u0026gt;check_clpath (line 656) Unable to find cl.exe 出错 vl_compilenn (line 426) cl_path = fileparts(check_clpath()); % check whether cl.exe in path 这里是缺少\u0026rsquo;cl.exe'，观察我所使用的vscode，发现其并没有\u0026rsquo;cl.exe'，从而安装了vs2019，将该文件的路径加到配置文件中去。详见 Matconvnet error cl.exe not found\n  问题三   旧版本的matlab，如2015b，如果后安装的vs，配置文件里是没有相关的文件的，在max -setup -V 的时候能发现不会去寻找该编译器。经尝试，将高版本已有的配置文件选择好vs的版本，复制过去是可行的。\n","date":"2021-09-26T19:16:13+08:00","permalink":"https://deathsprout.github.io/p/%E9%87%8D%E8%A3%85%E7%94%B5%E8%84%91%E5%AF%BC%E8%87%B4matlab%E9%81%87%E5%88%B0%E7%9A%84%E4%BA%8C%E4%B8%89%E4%BA%8B/","title":"重装电脑导致matlab遇到的二三事"},{"content":"关键字：神经可塑性，认知可塑性，功能可塑性，突触可塑性，短时突触可塑性， LTP，稳态可塑性\n神经科学领域，可变性或可修饰性被称为可塑性（Plasticity），可塑性是神经 系统的重要特性，很少用于生命科学的其他领域。神经可塑性的表现形式涉及多 个层次，也涉及多种多样的影响因素，可以说神经可塑性的内涵非常广泛，从宏 观上遵循特定模式形成的行为逻辑，到微观上分子作用导致的结构变化等等 [1]。简要的说，神经可塑性是包括了神经组合形式改变、神经活动方式变化、神经发 育演化、神经损伤修复等一系列体现了生物体神经系统的适应性的自然属性。\n神经可塑性就物理尺度上可以分成三个层次，宏观层次、细胞层次（介观层 次）、分子层次（微观层次）。宏观层次上就影响对象还可以分成:涉及特定脑区， 影响个体生物信息加工和行为逻辑的认知可塑性;涉及非特定脑区，脑损伤后引 起功能重组的功能可塑性;以及功能重组过程中结构改变表现出的结构可塑性。 细胞层次上，神经可塑性的主要表现形式是突触可塑性，具体又可划分为突触传 递的可塑性（易化、压抑、PTP、长时程增强（Long-term potentiation（LTP））、 长时程抑制（Long-term depression（LTD）、PHP、突触缩放等等）和突触再可塑 性（描述可塑性的可塑性）。分子层次主要指神经可塑性的分子机制，大体包括 参与突触可塑性的蛋白分子和离子、形成神经可塑性的信号通路、控制神经系统 发育的遗传物质。\n宏观水平上的神经可塑性 认知可塑性 对认知可塑性的概念向前追溯，甚至可以追溯到亚里士多德在《论灵魂》中 的蜡块说，随着科技的进步，人们也意识到神经是认知活动的主要载体，但在 Hebb 提出神经学习理论之前，这些认知更多的是没有理论框架的猜想和纯粹事 实[2]。Hebb 在他的书《行为的组织》中，提出现在称为赫布定律的假设:“当神 经元A 的轴突与神经元B 很近并重复持续的参与了对B 的兴奋时，这两个神经元 或其中一个便会发生某些生长过程或代谢变化，致使A使B兴奋的效能增强了。” [3] 在后来，有相当多的研究从分子和细胞层面证明了这一假设的正确性 [4]，Markram 在赫布理论的基础上，提出了“脉冲时间依赖可塑性（STDP）”的理论， 通过神经元激发的时间顺序来影响神经元之间的联系的强弱，即突触对时间信号 具有敏感性，时间仅在数十毫秒内，正相关的突触后放电会引起突触联系增强， 负相关则相反 [5] 。\n根据赫布理论和后续的大量研究，对脑的记忆、知觉等功能的认识逐渐丰富。 就记忆形成方面来说，记忆是外界刺激对神经元集合作用的结果，以神经元集合 间突触强度改变的形式分布于大脑中，在记忆形成的过程中，神经元结构进行快 速的重构，神经元间会以快速的协同放电来加强相关神经元之间的连接，而这种 连接是广泛存在于大脑，包括纹状体、杏仁核、前额叶、小脑和内侧颞叶等，并 非集中于某个区域，但不同的解剖区域储存的记忆的类型是不同的，比如前额叶 脑区与工作记忆有关，海马与空间记忆高度相关等 [6] 。\n神经可塑性在行为方面主要体现为条件反射上，这是关联性学习，由于在记 忆提取时会激活形成该记忆的相关连接和脑区 [7] ，可以将条件反射视为通过学习 产生的记忆中的一种。经典条件反射指将正常不引起反应的条件刺激和会引起某 种反应的非条件刺激（比如食物（非条件刺激）对于分泌唾液）在时序上联系起 来。其他还有操作式条件反射（将一种动作与有意义刺激联系起来）、评价性条 件反射 [8] （当一条件刺激与另一积极或消极刺激反复联系起来，被试者对原刺激 的喜好发生改变）等类型。\n功能可塑性和结构可塑性 功能可塑性体现在神经系统功能重组的过程中，功能重组指神经系统损伤后， 有一定能力去进行结构改变和功能代偿，进而代替或部分代替受损区域原有功能 [9] 。虽然发育成熟的神经细胞不具备增殖和分裂能力，但神经元会持续具有形成 新的突触连接和修饰自身显微形态的能力。按照损伤发生的部位，可以将功能可 塑性划分为中枢神经系统功能可塑性和脊髓功能可塑性。\n相对于中枢神经系统，脊髓的功能可塑性单一且较弱，主要表现为不完全损 伤后出芽，主要包括再生性出芽（regenerating sprouting，轴突受损的神经元存活， 近侧段长出新芽）、侧枝出芽（lateral sprouting，整个神经元死亡，附近未受伤神 经元从侧枝上长出新芽）、代偿性出芽（compensatory sprouting，发育中神经元轴 突部分侧枝死亡，正常侧枝发出新芽），而完全性损伤后运动功能的恢复机制尚 不明确 [10] 。\n \n图1 功能重组示意图 对于中枢神经系统来说，在1969 年，Luria 等人提出并完善了功能重组理论 （也被称为再训练理论），认为脑损伤后的余下部分，通过在功能上的重新组合 实现在损伤中丧失的功能 [11] 。细胞层次上的突触可塑性是损伤后参与 脑功能重组的重要神经生理过程，抗稳态和稳态可塑性都可能促进损伤后的神经 网络功能重组 [12，13] ，特别是LTP 的效率，即突触增强的效率，对神经功能重组有 重要影响，甚至直接与临床恢复程度相关 [14] 。神经元间突触连接广泛增加是应对 各种类型损伤的常见反应，可能有助于去适应因损伤造成的神经元连接数量下降， 以此来部分恢复神经网络功能，延缓症状的出现 [15] 。在神经功能重塑早期，突触 连接的增加是过量的，可以广泛的引起神经兴奋过度，进而使现存神经网络连通 性增强，有利于提升未损伤神经网络效率 [15] 。在神经功能重塑后期，根据稳态可 塑性和STDP 的双向调节，有效的降低过量的突触连接，选择性的保留并增强活 跃的突触连接，实现功能和结构的重塑 [15] 。\n分子细胞水平上的神经可塑性 本部分主要介绍神经可塑性中最重要的突触可塑性，鉴于属于细胞水平的表 象和属于分子水平的机制不适合分开阐述，故将两个层次上的神经可塑性放在一 起。\n突触可塑性及机制 突触可塑性是经验依赖性可塑性，这些突触多为化学性突触，其表现与个体 神经元历史活动高度相关，常规意义上的突触可塑性特指由LTP 或LTD 表现出 的经验依赖性的突触效能改变。这里将之含义扩大，除了突触传递的可塑性（比 如LTP、LTD、短时程突触可塑性、突触缩放等），还包括突触再可塑性 [16，17，18] 。\n短时突触可塑性 从简单的无脊椎动物到哺乳动物，几乎每一个突触都可以观察到多种形式的、 持续时间从毫秒到几分钟不等的短期突触可塑性 [19] ，表现为突触易化、突触压抑、 强直后增强这些形式。突触后电位在相邻数百毫秒的时间间隔上对后续刺激反应 的增强，叫突触易化（synaptic facilitation），相反，对后续刺激反应减弱叫突触 压抑（synaptic depression）。长串的高频刺激往往会先引起突触压抑，数秒后再 形成持续数分钟乃至数十分钟的突触后电位增强，被称为强直后增强（posttetanic potentiation） [20] 。\n在研究的所有突触中，短时突触可塑性都被证明是突触前机制引起的，易化、 增强和强直后增强具体表现是统计上动作电位释放的递质量子数量增加，没有突 触后的效应，作为量子的囊泡本身大小也没有变化，在统计上表现出的数量增加 反应了突触前囊泡预备池中释放概率的增加 [19] 。Katz 和Miledi（1968）认为可能 在初次刺激后仍有Ca2+ 存在于突触前神经元中，并导致后续更大的钙信号引起突 触强度变化 [21] ，这在后来完善成为残留钙假说，并有诸多研究观察到与该假说一 致的促进作用。Ca2+ 离子在此过程中至关重要，囊泡融合的核心是由syt1 （syntaxin-1）、突触囊泡蛋白和SNAP-25 形成复合物共同将囊泡和质膜相连，而 复合物SNARE 对Ca2+ 敏感，Ca2+ 浓度提升能促进囊泡释放 [22] 。Schneggenburger 和Neher（2000）研究发现在大型突触末端的杯状突起处，仅10μM 幅度的Ca2 + 的阶梯状升高诱导了快速的递质释放，能在3ms 内消耗大约80％的囊泡预备池 [23] ，表明Ca2 + 的短时增多能触发神经递质的快速释放。在短时间内，初次刺激过 后突触前神经元Ca2 + 浓度上升，产生易化，但过多的Ca2 + 将囊泡预备池耗竭， 引起短时间的压抑，而高频刺激先引起突触压抑，后续的强直后增强对应的是囊 泡预备池的增大（以及Ca2 + 的积聚） [20] 。\n突触可塑性\u0026ndash;LTP 突触可塑性最受关注的现象是长时程增强（LTP）和长时程抑制（LTD），尤 其是LTP，自从1973 年在海马中发现长时程增强以来 [24] ，大量关键相关论文发 表，LTP 目前为学习和记忆提供了一个有说服力的细胞模型。经历高频刺激的突 触，其会产生长达数小时甚至数天的兴奋性突触电位幅度增强，这种现象被称为 长时程增强。后续的研究发现LTP 有多种特性，McNaughton（1978）和Levy and Stewart（1979）先后报道LTP 具有“协同性（（1）该突触的前神经元兴奋并释放 神经递质;（2）该突触后神经元处于去极化状态。某突触的LTP 可以在上述这两 种条件同时存在时产生）”和“关联性（强刺激作用后，一定时间内一个弱刺激 也能引发LTP 的性质）”，以及“输入特异性（LTP 只发生在经历刺激的突触上， 不发生在同一突触后神经元的其他未受刺激的突触上）”，这些发现将LTP 视为 赫布假设实现的过程 [25] 。接着一方面通过向突触后神经元注入去极化电流 （Gustafsson，1987），一方面通过超极化等方式防止LTP 期间产生去极化电位 （Malinow、Miller, 1986），证明了产生LTP 不需要一定存在高频刺激，只有两 个硬性要求:突触刺激和突触后去极化 [25] 。这些特性使得LTP 适用于调节突触连 接网络，动物的个体经历能够在单个突触水平上改变所在突触连接网络上每个突 触的强度，比如输入特异性能使独立调控突触后神经元与不同突触前神经元形成 的连接强度成为可能;协同性可以使单一输入信号同时去极化复数突触后神经元， 调节一组突触连接的强度;关联性使同时的输入信号之间可以互相影响 [26] 。大多 表现LTP 的突触也表现一种或多种形式的LTD，其机制和效应虽然不同，但LTD 与LTP 两者都描述了一类现象。\n目前认为LTP 的分子机制（见图2）同时涉及突触前和突触后的因素，不过 突触前因素没有突触后因素重要，根据LTP 的过程可以划分为LTP 的诱导、维 持和表达三个阶段，根据维持的持续时间又将LTP 分成E-LTP（Early Phase LTP） 和L-LTP（Late Phase LTP）。\nLTP 和LTD 的突触前因素除了部分与短时效应类似，表现在突触前神经元 神经递质释放量的增加/减少上之外 [20] ，一些突触的突触后神经元会有一个逆行 信号“NO”促进突触前神经元释放神经递质 [25] 。突触后因素中，NMDA 受体对 LTP 的诱导至关重要，如果敲除掉NMDAR1 基因，LTP 无法发生 [27] 。NMDA 型 谷氨酸受体在神经元处于静息电位时被Mg2+ 占据，而在去极化时与Mg2+ 分离（突 触后去极化），此时NMDA 受体与神经递质谷氨酸结合（突触刺激），能引起Ca2+ 内流，这是LTP 的诱导过程。\nE-LTP 的刺激量小，持续时间短，这个过程没有新蛋白质的合成，而L-LTP 刺激量大，产生后维持的持续时间能超过24 小时，会有cAMP、PKA 等的参与， 最后合成蛋白质实现表达。钙离子内流会激活两个关键的信号蛋白，CaMKII 和 蛋白激酶PKC。钙与钙调蛋白结合激活CaMKII 后，CaMKII 磷酸化细胞膜上的 AMPA 型通道受体，增强其通道电导，使得突触后膜上的AMPA 受体对刺激反 应幅度更大 [28] 。这些AMPA 受体并非完全位于细胞膜上，沉默突触的发现表明 在LTP 发生之前，突触后的终扣只有部分有AMPA 受体，都有NMDA 受体但是 被Mg2+ 堵塞，而激活的蛋白激酶PKC 和CaMKII 能导致含有AMPA 受体的囊泡 与细胞膜融合，将AMPA 受体插入突触后膜上，CaMKII 还能将AMPA 的结合 蛋白stargzin 磷酸化，使stargzin 结合PSD95，引导AMPA 受体处在突触后膜区 域，使该突触的效能上升 [28] 。这里的PSD（postsynaptic density）指的是突触后致 密区域，根据Bosch 等人的研究，PSD 在LTP 诱导后一个小时左右出现，PSD 的形成会改变突触可塑性的规则，这可能能解释为什么突触可塑性的规则会随时 间改变 [29] 。L-LTP 在此基础上，强烈的钙离子内流还会激活存在于突触旁的 mRNA 翻译产生PKMζ，该蛋白再磷酸化囊泡上的AMPA 蛋白。这是LTP 的维 持过程，CaMKII 能够自身磷酸化，在没有钙离子存在的情况下也能长时间保持 活性，而PKMζ没有调控域，只靠亚单位催化，在激活后无法自主的失活，同 样因为其能调控突触位置蛋白的翻译，能在没有钙离子存在的情况下保持较高的 水平 [28] 。\n \n图2 LTP分子机制示意图 值得注意的是，CaMKII 被发现占据脑蛋白量的2%[ 30] ，在突触中调节许多 的过程，应当有严格调节CaMKII 的机制存在，可能涉及两种内源性的分子抑制 剂CaMK2N1 和CaMK2N2[ 31] 。在一项新的研究中，研究者发现在LTP 诱导产生 后，CaMK2N2 基因呈现分级上调，意味着CaMK2N2 蛋白可能会在LTP 过程中 发挥作用，也许会参与形成突触的再可塑性 [32] 。\nLTP 的最后一个阶段对应蛋白质性质改变，在L-LTP 中还有新蛋白质合成 和新突触的形成。重复刺激引起的钙离子信号激活腺苷酸环化酶，进而激活 cAMP/PKA 信号途径，PKA 的调节亚基脱落后，PKA 的催化亚基进入细胞核， 磷酸化CREB-1 转录因子，开启部分相关基因的表达。\n对LTP 的影响因素还有很多，Cortese 等人发现丰富的环境提高了不同行为 任务中探索记忆和学习的表现，而且丰富的环境可以增强老年大鼠的mGluR5 依 赖性LTP[ 33] 。\n稳态可塑性 另外突触传递的可塑性形式还有稳态可塑性，突触前稳态可塑性（PHP），主 要是由于神经递质受体功能或动作电位活性受损引起的，神经递质受体水平的绕 动都会导致相应神经递质释放的增多 [34] 。谷氨酸受体干扰实验中神经递质释放增 加精确的补偿了施加的干扰 [35] ，表明PHP 过程含有逆行的跨突触信号系统 [34] ， 但目前对PHP 机制的了解不多，2017 年有实验组发现信号素2b 作为突触前 plexin B 的受体，形成缩写为Sema2b–PlexB 的跨突触信号传导通路 [36] 。\n突触后稳态可塑性表现形式是神经递质受体的稳态调节，即突触缩放。突触 缩放这一现象，具体是指神经元细胞在特定条件下会调节细胞上所有突触连接的 强度，不同突触之间的相对效能不发生变化，以此响应长时间的变化 [37] 。突触后 神经元上活动总体的短期持续下降会导致突触缩放，总突出强度净增长，长期不 活动会导致相反的趋势，而如果将一个神经元的全部输入阻断，该神经元表现出 超敏感性，可兴奋性和突触输入增强 [37] 。\n参考文献 [1]吴馥梅.脑神经可塑性[J].现代特殊教育,1999(06):12-13.\n[2] 黄家裕. 认知神经的可塑性:赫布理论的哲学意蕴[J]. 哲学动态, 2015, 000(009):104-108.\n[3] Hebb, D. (1949)The Organisation of Behaviour, Wiley\n[4] Bi, G. and Poo, M. (2001) Synaptic modification by correlated activity:Hebb’s postulate revisited.Annu. Rev. Neurosci.24, 139–166\n[5] Roberts P D , Bell C C . Spike timing dependent synaptic plasticity in biological systems[J]. Biological Cybernetics, 2002, 87(5-6):392-403.\n[6] Howard Eichenbaum 著，周仁来等译，杨治良审校:记忆的认知神经科学\u0026ndash;导 论。北京师范大学出版社\n[7] A R C , A F D , A S E P , et al. Attention-related activity during episodic memory retrieval: a cross-function fMRI study[J]. Neuropsychologia, 2003, 41( 3):390-399.\n[8] De Houwer J , Thomas S , Baeyens F . Associative learning of likes and dislikes: A review of 25 years of research on human evaluative conditioning[J]. Psychological Bulletin, 2001, 127(6):853-869.\n[9] Ward N S . Mechanisms underlying recovery of motor function after stroke[J]. Postgraduate Medical Journal, 2005, 81(958):510-.\n[10] 肖志峰,陈冰,赵燕南,李佳音,韩素芳,陈艳艳,戴建武.脊髓损伤再生研究进展 ——搭建脊髓损伤修复的希望之桥[J].中国科学:生命科学,2019,49(11):1395-1408.\n[11] 缪鸿石. 中枢神经系统(CNS)损伤后功能恢复的理论(一)[J]. 中国康复理论 与实践, 1995, 001(001):1-4.\n[12] Desai, N.S.; Cudmore, R.H.; Nelson, S.B.; Turrigiano, G.G. Critical periods for experience-dependent synaptic scaling in visual cortex. Nat. Neurosci. 2002, 5, 783– 789.\n[13] Turrigiano, G. Homeostatic synaptic plasticity: Local and global mechanisms for stabilizing neuronal function. Cold. Spring Harb. Perspect. Biol. 2012, 4, a005736.\n[14] Mori, F.; Kusayanagi, H.; Nicoletti, C.G.; Weiss, S.; Marciani, M.G.; Centonze, D. Cortical plasticity predicts recovery from relapse in multiple sclerosis. Mult. Scler. 2014, 20, 451–457.\n[15] Stampanoni Bassi, M.; Iezzi, E.; Gilio, L.; Centonze, D.; Buttari, F. Synaptic Plasticity Shapes Brain Connectivity: Implications for Network Topology. Int. J. Mol. Sci. 2019, 20, 6193.\n[16] Citri A , Malenka R C . Synaptic Plasticity: Multiple Forms, Functions, and Mechanisms[J]. Neuropsychopharmacology, 2008, 33(1):18-41.\n[17] Singh P, Heera PK, Kaur G.Expression of neuronal plasticity markers in hypoglycemia induced brain injury[J].Mol Cell Biochem2003, 247 (1) :69-74.\n[18] 王韶莉,陆巍.再可塑性在学习记忆中作用的研究进展[J].生理学 报,2016,68(04):475-482.\n[19]Zucker R S , Regehr W G . SHORT -TERM SYNAPTIC PLASTICITY[J]. Annual Review of Physiology, 2002, 64(1):355-405.\n[20]Nicholls J G . From Neuron to Brain[M]. Sinauer Associates, 2001.\n[21]Katz B, Miledi R. 1968. The role of calcium in neuromuscular facilitation. J. Physiol. 195:481–92\n[22]Jackman S L , Regehr W G . The Mechanisms and Functions of Synaptic Facilitation[J]. Neuron, 2017, 94(3):447-464.\n[23]Schneggenburger R , Neher E . Intracellular calcium dependence of transmitter release rates at a fast central synapse[J]. Nature, 2000, 406(6798):889-893.\n[24]Mechanisms[J]. Neuropsychopharmacology, 2008, 33(1):18-41.\n[25] Nicoll, Roger A . A Brief History of Long-Term Potentiation[J]. Neuron, 2017, 93(2):281-290.\n[26]Liqun Luo. Principles of Neurobiology[J]. crc press, 2015.\n[27]Tsien, J. Z., Huerta, P. T., \u0026amp; Tonegawa, S. (1996). The essential role of hippocampal CA1 NMDA receptor–dependent synaptic plasticity in spatial memory.Cell,87(7), 1327-1338.\n[28]Lisman J , Yasuda R , Raghavachari S . Mechanisms of CaMKII action in long- term potentiation[J]. Nature Reviews Neuroscience, 2012, 13:2358.\n[29]Bosch M , Castro J , Saneyoshi T , et al. Structural and Molecular Remodeling of Dendritic Spine Substructures during Long-Term Potentiation[J]. Neuron, 2014, 82(2):444-459.\n[30]Nikolai, Otmakhov, and, John, \u0026amp; Lisman. Measuring CaMKII concentration in dendritic spines[J]. Journal of Neuroscience Methods, 2012.\n[31] B.H. Chang, S. Mukherji, T.R. Soderling. Calcium/calmodulin-dependent protein kinase II inhibitor protein: localization of isoforms in rat brain[J]. Neuroscience, 2001, 102(4):0-777.\n[32]Sanhueza, M.CaMKII inhibitor 1 (CaMK2N1) mRNA is upregulated following LTP induction in hippocampal slices[J]. Neurosciences , 2020.\n[33]Cortese G P , Olin A , O’Riordan, Kenneth, et al. Environmental Enrichment Improves Hippocampal Function in Aged Rats by Enhancing Learning and Memory, LTP and mGluR5-Homer1c Activity[J]. Neurobiology of Aging, 2017:S0197458017303731.\n[34] Delvendahl I , Müller, Martin. Homeostatic plasticity—a presynaptic perspective[J]. Current Opinion in Neurobiology, 2019, 54:155-162.\n[35]Frank CA, Kennedy MJ, Goold CP, Marek KW, Davis GW: Mechanisms underlying the rapid induction and sustained expression of synaptic homeostasis. Neuron 2006, 52:663-677.\n[36] Orr B O , Fetter R D , Davis G W . Retrograde semaphorin-plexin signalling drives homeostatic synaptic plasticity[J]. Nature, 2017, 550(7674):109-113.\n[37] Citri A , Malenka R C . Synaptic Plasticity: Multiple Forms, Functions, and Mechanisms[J]. Neuropsychopharmacology, 2008, 33(1):18-41.\n","date":"2021-06-25T20:54:26+08:00","image":"https://s2.loli.net/2021/12/05/EAlSkpHZeyqDF24.png","permalink":"https://deathsprout.github.io/p/%E7%A5%9E%E7%BB%8F%E5%8F%AF%E5%A1%91%E6%80%A7/","title":"神经可塑性"},{"content":"神经生物   中枢神经系统 central nervous system CNS\n  脑 brain\n 大脑皮质 cerebral cortex 基底节 basel ganglia 海马 hippocampus 杏仁核 amygdala 丘脑 thalamus 下丘脑 hypothalamus 小脑 cerebellum 脑干 brainstem  中脑 midbrain 脑桥 pons 延髓 medulla      脊髓 spinal cord\n    神经解剖学 neurianatomical 组织学切面\n 冠状切面 coronal section：又名横截面，拦腰的那种 矢状切面 sagittal section： 从头劈到尾，左右分开 水平切面 horizontal section：字面意思    神经节 ganglia（ganglion（组织、医学范围，说神经节细胞一般是ganglion cell））：神经元集群\n  神经元 neuron\n 胞体 soma 神经突起 neuronal process  树突 dendrite  树突棘 dendritic spine ：树突上的小突起   轴突 axon  突触前末梢 presynaptic        神经胶质细胞 glia\n 少突胶质细胞 oligodendrocyte 施万细胞 schwann cell 星型胶质细胞 astrocyte 小胶质细胞 microglia     白质 white matter ：由少突胶质细胞和有髓轴突共同组成，髓鞘（myelin sheath）中富含脂质，呈现白色。 灰质 gray matter    突触 synapse\n 化学突触 chemical synapse  神经递质 neurotransmitter  乙酰胆碱 acetylcholine ACh   突触间隙 synaptic cleft 突触囊泡 synaptic vesicle 突触后特化 postsynaptic specialization （也称突触后致密 postsynaptic density）   电突触 electrical synapse  间隙链接 gap junction  连接子蛋白 connexin        锥体神经元 pyramidal neuron\n  动态极化理论 theory of dynamic polarization\n  膜电位 member potential\n  神经冲动 nerve impulse\n  电位\n 动作电位 action potential 分级电位 graded potential（也称局部电位）  突触电位 （在突触后膜对神经递质的响应位点上产生）  突触传递 synaptic transmission ：   受体电位 receptor potential      传入神经 afferent\n  传出神经 efferent\n   兴奋性神经元 excitatory neuron 抑制性中间神经元 inhibitory interneuron 投射神经元 projection neuron ：轴突将神经系统两个不同区域连接在一起    回路基序\n 前馈兴奋 feedforward excitation 前馈抑制 feedforward inhibition 反馈抑制 feedback inhibition 复发性抑制（交叉抑制） recurrent（cross） inhibition 侧抑制 lateral inhibition 去抑制 disinhibition      脑沟 fissure\n 额叶 frontal lobe 顶叶 parietal lobe 颞叶 temporal lobe 枕叶 occipital lobe    拓扑映像 topographic map\n  扰动实验 perturbation experiment\n 功能失去实验 loss-of-function experiment 功能获得实验 gain-of-function experiment    离子通道 ion channel\n  电化学梯度 electrochemical potential\n  被动运输 passive transport\n  通透性 permeability\n   阳离子 cation 阴离子 anion    峰电位 spike （动作电位 action potential）\n  阈值 threshold\n 阈下刺激 subthreshold stimulus 阈上刺激 suprathreshold stimulus    不应期 refractory period ：K+离子通道激活延迟和Na+离子通道失活导致的，不能从头产生另一个动作电位的时期。\n  跳跃式传导 saltatory conduction ：有髓轴突的动作电位会在节点之间跳跃前进\n 髓鞘 myelin sheath 郎飞结 nodes of Ranvier       突触易化 synaptic facilitation 突触压抑 synaptic depression 强直后增强 posttetanic potentiation    河豚毒素 tetrodotoxin TTX ： 有效阻断电压门控的Na离子通道\n  膜片钳记录技术 patch clamp recording\n  突触结合蛋白 synaptotagmin\n  存储池 reserve pool\n  易释放池 readily releasable pool\n  容积传递 volume transmission ：神经递质可能在突触外的胞外空间影响周围多个细胞\n   空间整合 spatial integration 时间整合 temporal integration    视神经 optic nerve\n 感光细胞 photoreceptor  视杆细胞 rod 视锥细胞 cone 中央凹   视网膜节细胞 retinal ganglion cell RGC ：输出层，通过轴突将信息传递给大脑，轴突成束即视神经 双极细胞 bipolar cell ：将信号从感光细胞传递到RGC 水平细胞 horizontal cell      发色团 chromophore\n  侧抑制 lateral inhibition\n  树突平铺 dendritic tiling ：许多种视网膜节细胞和无长突细胞的树突布满视网膜的同时彼此没有重叠\n  对立颜色视网膜节细胞 color-opponent RGC ： 蓝-黄、红-绿\n   顶盖 tectum ： 两栖和低等脊椎动物RGC在脑中的目标脑区\n  视网膜拓扑映射 retinotopy\n  内源信号成像 intrinsic signal imaging ： 利用代谢特征，包括血流速和血氧水平来反应神经元活动。\n  眼优势 ocular dominance ：许多输入皮质中第4层的皮质神经元只对来自一只眼的输入有很强的偏好性。\n  梭状脸部区域 fusiform face area ：一个优先被人脸图像激活的区域。\n  带缩写的视觉相关区域\n 外侧膝状体核 lateral geniculate nucleus LGN 颞中视觉区 middle temporal visual area MT ：MT神经元对于运动方向特别敏感 外侧顶内沟区 lateral intraparietal area LIP：放电的频率能预料眼将要运动的方向，LIP神经元活动与决定下达有因果关系。    轴突导向分子 axon guidance molecule\n  视交叉 optic chiasm ：所有脊椎动物都有大量RGC轴突穿越中轴线，投射到异侧脑\n 对侧 contralaterally 投射：人鼻侧视网膜轴突穿过中轴线（60%） 同侧 ipsilateral ：颞侧（40%）    气体分子 odorant\n  嗅纤毛 olfactory cilium\n  嗅球 olfactory bulb\n  多态性 polymorphism\n  感知对象 percept\n  原位杂交 in situ hybridization ：用核酸探针在整块组织上探测基因表达\n  等位排斥 allelic exclusion ：特定ORN中，起功能的气味受体的mRNA仅仅转录自一对同源染色体的其中一个。\n   僧帽细胞 mitral cell 簇绒细胞 tufted cell 梨形皮质 piriform cortex 投射神经元 projection neurin PN ：嗅觉信息由PN转寄到更高的嗅觉中枢 局部中间神经元 local interneuron LN ：    增益控制 gain control ：指通过调节系统输入输出的关系，使输出限制在有限动态范围内的过程。\n  逆向跨突触追踪 retrograde trans-synaptic tracing ：标记给定神经元发送直接信号的突触前神经元\n  定向的、面向\u0026hellip;的 oriented : oriented bars\n   计算神经科学  反向传播 back propagation 主成分分析 principal component analysis 编码空间 coding space 基于规范 Norm-Based Hierarchical Model and X (HAMX): 层次、等级制度 hierarchy 调谐 tuned 重铸 recast 稀疏 sparseness 表现超过、性能优于 outperformed 连通性 connectivity 封闭性 closedness 不透明的、难理解的、晦涩的 opaque  认知神经科学    论文中杂项  信条 tenet 基本机制 elementary mechanisms 专门、具体的 particularly 可论证的 arguably 可观、显著的、实质性的 substantial 说明的、解释的 explanatory 命名系统、术语集 nomenclature  术语 terminology   公开的 overt 范例 paradigm    昏迷 comatose  弥漫的 diffuse 假定 postulates 交织 intertwined 隐含的 underlying 特性、财产 properties 杠杆的、杠杆作用 leveraged 老化 aging 精神病(学)的 psychiatric 类似于 akin to 复杂的 sophisticated 死胡同 cul-de-sac 统一 unifying 粗略的、粗糙的 coarsely 精细尺度 finer-scale 为了便于说明 For ease of exposition 艺术级的、尖端水平的、最先进的 state-of-the-art 妨碍 hampered 变形的 morphing 精细的；精巧的 subtler 负责的；易控制的；经得起检验的 amenable 退化 degrad 处于个人的原因 for its own sake  物种专有名词  猕猴 macaque  ","date":"2021-06-23T12:46:06+08:00","permalink":"https://deathsprout.github.io/p/%E7%A5%9E%E7%BB%8F%E7%94%9F%E7%89%A9%E5%92%8C%E8%AE%A1%E7%AE%97%E7%A5%9E%E7%BB%8F%E8%8B%B1%E6%96%87%E5%90%8D%E8%AF%8D/","title":"神经生物和计算神经英文名词"},{"content":"问题如下： |使用Python或者Perl编程，完成以下任务：\n 1.tab是Trichoderma reesei QM6a的annotation文件，2.obo是GO的ontology文件，根据ontology文件中所记录的GO关系：  (a)\t获得已注释的所有基因属于哪一个GO的全部列表\n(b)\t计算Trichoderma reesei QM6a中每一个GO分别有多少个基因。\nDEG.txt中记录了某一次转录组分析的差异基因结果。请计算：  (a)\t每一个GO中所属的基因显著上调和下调的个数有多少？\n(b)\t哪些GO发生了显著的上调或者下调的富集？\n文件解释： 先说一下文件，1.tab是如下图所示的annotation文件，大概两万多行，有proteinId，gotermId，goName，gotermType，goAcc的信息，其中本次用到的的是proteinId和goAcc。goAcc是该基因所属的GO号，形式是GO:加七位数字。一个基因能对应多个GO id，一个GO id也能对应多个基因。\nproteinId\tgotermId\tgoName\tgotermType\tgoAcc 1673\t3504\ttranslational initiation\tbiological_process\tGO:0006413 1673\t2855\tcytoplasm\tcellular_component\tGO:0005737 1673\t1108\ttranslation initiation factor activity\tmolecular_function\tGO:0003743 1673\t1096\tRNA binding\tmolecular_function\tGO:0003723 1692\t3503\tprotein biosynthesis\tbiological_process\tGO:0006412 1692\t2953\tribosome\tcellular_component\tGO:0005840 1692\t2746\tintracellular\tcellular_component\tGO:0005622 1692\t1107\tstructural constituent of ribosome\tmolecular_function\tGO:0003735 1692\t1096\tRNA binding\tmolecular_function\tGO:0003723 1702\t3444\ttranscription\tbiological_process\tGO:0006350 1702\t1233\tDNA-directed DNA polymerase activity\tmolecular_function\tGO:0003887 1702\t1056\tDNA binding\tmolecular_function\tGO:0003677 1702\t10782\tsigma DNA polymerase activity\tmolecular_function\tGO:0019984 1702\t8197\ttheta DNA polymerase activity\tmolecular_function\tGO:0016452 1702\t8196\tnu DNA polymerase activity\tmolecular_function\tGO:0016451 1702\t8195\tkappa DNA polymerase activity\tmolecular_function\tGO:0016450 1702\t8194\tlambda DNA polymerase activity\tmolecular_function\tGO:0016449 1702\t8193\tmu DNA polymerase activity\tmolecular_function\t2.obo记录了GO之间的上下级关系，文件本身三十几万行，47125个GO，如下所示的[Term]表示一个GO，随后id：GO：XXXXXXX就是这个GO的 id，namespace就是这个GO属于本体论里三个顶级GO的哪一个，比如GO:0000001属于biological_process。\n第一题的（a）就是要找到所有GO的所有上级GO（或者说父亲），最终归宿到biological_process、molecular_function、cellular_component。\nformat-version: 1.2 data-version: releases/2018-04-12 subsetdef: goantislim_grouping \u0026#34;Grouping classes that can be excluded\u0026#34; subsetdef: gocheck_do_not_annotate \u0026#34;Term not to be used for direct annotation\u0026#34; subsetdef: gocheck_do_not_manually_annotate \u0026#34;Term not to be used for direct manual annotation\u0026#34; subsetdef: goslim_agr \u0026#34;AGR slim\u0026#34; subsetdef: goslim_aspergillus \u0026#34;Aspergillus GO slim\u0026#34; subsetdef: goslim_candida \u0026#34;Candida GO slim\u0026#34; subsetdef: goslim_chembl \u0026#34;ChEMBL protein targets summary\u0026#34; subsetdef: goslim_generic \u0026#34;Generic GO slim\u0026#34; subsetdef: goslim_goa \u0026#34;GOA and proteome slim\u0026#34; subsetdef: goslim_metagenomics \u0026#34;Metagenomics GO slim\u0026#34; subsetdef: goslim_mouse \u0026#34;Mouse GO slim\u0026#34; subsetdef: goslim_pir \u0026#34;PIR GO slim\u0026#34; subsetdef: goslim_plant \u0026#34;Plant GO slim\u0026#34; subsetdef: goslim_pombe \u0026#34;Fission yeast GO slim\u0026#34; subsetdef: goslim_synapse \u0026#34;synapse GO slim\u0026#34; subsetdef: goslim_virus \u0026#34;Viral GO slim\u0026#34; subsetdef: goslim_yeast \u0026#34;Yeast GO slim\u0026#34; subsetdef: gosubset_prok \u0026#34;Prokaryotic GO subset\u0026#34; subsetdef: mf_needs_review \u0026#34;Catalytic activity terms in need of attention\u0026#34; subsetdef: termgenie_unvetted \u0026#34;Terms created by TermGenie that do not follow a template and require additional vetting by editors\u0026#34; subsetdef: virus_checked \u0026#34;Viral overhaul terms\u0026#34; synonymtypedef: syngo_official_label \u0026#34;label approved by the SynGO project\u0026#34; synonymtypedef: systematic_synonym \u0026#34;Systematic synonym\u0026#34; EXACT default-namespace: gene_ontology remark: cvs version: use data-version remark: Includes Ontology(OntologyID(Anonymous-35)) [Axioms: 230 Logical Axioms: 228] remark: Includes Ontology(OntologyID(OntologyIRI(\u0026lt;http://purl.obolibrary.org/obo/go/never_in_taxon.owl\u0026gt;))) [Axioms: 18 Logical Axioms: 0] ontology: go [Term] id: GO:0000001 name: mitochondrion inheritance namespace: biological_process def: \u0026#34;The distribution of mitochondria, including the mitochondrial genome, into daughter cells after mitosis or meiosis, mediated by interactions between mitochondria and the cytoskeleton.\u0026#34; [GOC:mcc, PMID:10873824, PMID:11389764] synonym: \u0026#34;mitochondrial inheritance\u0026#34; EXACT [] is_a: GO:0048308 ! organelle inheritance is_a: GO:0048311 ! mitochondrion distribution [Term] id: GO:0000002 name: mitochondrial genome maintenance namespace: biological_process def: \u0026#34;The maintenance of the structure and integrity of the mitochondrial genome; includes replication and segregation of the mitochondrial chromosome.\u0026#34; [GOC:ai, GOC:vw] is_a: GO:0007005 ! mitochondrion organization [Term] id: GO:0000003 name: reproduction namespace: biological_process alt_id: GO:0019952 alt_id: GO:0050876 def: \u0026#34;The production of new individuals that contain some portion of genetic material inherited from one or more parent organisms.\u0026#34; [GOC:go_curators, GOC:isa_complete, GOC:jl, ISBN:0198506732] subset: goslim_agr subset: goslim_chembl subset: goslim_generic subset: goslim_pir subset: goslim_plant subset: gosubset_prok synonym: \u0026#34;reproductive physiological process\u0026#34; EXACT [] xref: Wikipedia:Reproduction is_a: GO:0008150 ! biological_process [Term] id: GO:0000005 name: obsolete ribosomal chaperone activity namespace: molecular_function def: \u0026#34;OBSOLETE. Assists in the correct assembly of ribosomes or ribosomal subunits in vivo, but is not a component of the assembled ribosome when performing its normal biological function.\u0026#34; [GOC:jl, PMID:12150913] comment: This term was made obsolete because it refers to a class of gene products and a biological process rather than a molecular function. synonym: \u0026#34;ribosomal chaperone activity\u0026#34; EXACT [] is_obsolete: true consider: GO:0042254 consider: GO:0044183 consider: GO:0051082 ...... ...... 这最后的is_a: GO：XXXXXXX! 意思是本GO是后面这个GO的一部分，是它的下级，relationship: part_of GO：XXXXXXX!是同样的意思。\n有些GO和其他GO什么关系都没有，最后有个is_obsolete: true，说明它过时了，同时还会在def:最前面加上 OBSOLETE 。\n还有一些其他的关系，在这里暂时无须明白里面的所有内容,可以去看看一文极速读懂 Gene Ontology，老师说找relationship: part_of、is_a:建关系即可。\n#DEG.txt中记录了某一次转录组分析的差异基因结果 gene_id\tA\tB\tp-value\tFDR\tsignificance 105313\t476.125\t2.01685\t0\t0\tDOWN 81082\t188.012\t0.388781\t4.44E-16\t0.000106687\tDOWN 70204\t30.8683\t0.0378778\t3.33E-15\t0.000106687\tDOWN 112031\t74.6035\t0.708632\t4.66E-15\t0.000106687\tDOWN 122511\t1065.56\t8.16165\t1.44E-14\t0.000106687\tDOWN 49274\t106.879\t1.05212\t1.44E-14\t0.000106687\tDOWN 68803\t186.04\t2.46745\t1.69E-14\t0.000106687\tDOWN 123914\t149.261\t1.65829\t3.82E-14\t0.000106687\tDOWN 59362\t56.5321\t0.138204\t4.22E-14\t0.000106687\tDOWN 105882\t195.885\t0.663057\t5.64E-14\t0.000106687\tDOWN 108676\t703.881\t2.29893\t1.20E-13\t0.000106687\tDOWN 60489\t1.33266\t157.422\t4.06E-13\t0.000106687\tUP 66935\t2.52345\t123.558\t2.10E-12\t0.000106687\tUP 76659\t214.616\t1.82799\t3.86E-12\t0.000106687\tDOWN 解决问题 获得已注释的所有基因属于哪一个GO的全部列表 #引入包 import os import numpy as np import pandas as pd import csv import re import time #一开始想减少耗时，把2.obo中用不到的，即除is_a:|relationship: part_of|id: 的行全删了。 with open(\u0026#39;2.obo\u0026#39;,\u0026#39;r\u0026#39;) as f: while True: line = f.readline() strline = str(line) h = re.match(\u0026#39;is_a: GO|relationship: part_of|id: GO\u0026#39;,strline) if h is not None : clean = open(\u0026#39;2_clean.obo\u0026#39;,\u0026#39;a\u0026#39;) clean.write(line) clean.close() if not line: break f.close() #字典法 def dictGO(): father_list =[] with open(\u0026#39;2_clean.obo\u0026#39;,\u0026#39;r\u0026#39;) as f: while True: line = f.readline() strline = str(line) m = re.match(\u0026#39;id: GO\u0026#39;,line) if m is not None: if len(father_list) != 0: dict_[term] = father_list father_list = [] term = re.search(\u0026#39;GO:\\d{7}\u0026#39;,line).group() else: h = re.match(\u0026#39;is_a: GO|relationship: part_of\u0026#39;,strline) if h is not None: father = re.search(\u0026#39;GO:\\d{7}\u0026#39;,line).group() father_list.append(father) if not line: break f.close() #迭代找爹 def finder(a): global num father = dict_.get(a) if father: father_list.extend(father) num = num + 1 for i in father: finder(i) else: num_list.append(num) num = 0 #构建字典 dict_ ={} dictGO() tab = pd.read_csv(\u0026#39;1.csv\u0026#39;) #读入1.obo数据，可以将之换成csv读入，反正能读成数据框就行 with open(\u0026#34;dict.csv\u0026#34;,\u0026#34;w\u0026#34;,newline=\u0026#39;\u0026#39;) as csvfile: writer = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;) writer.writerow([\u0026#34;GO_id,\u0026#34;,\u0026#34;father_num,\u0026#34;,\u0026#34;father_generation,\u0026#34;,\u0026#34;father_id\u0026#34;]) #\u0026#34;GO_id,\u0026#34;结尾的逗号作为csv的分隔符 for j in range(0,len(tab[\u0026#39;goAcc\u0026#39;])-1): a = tab[\u0026#39;goAcc\u0026#39;][j] father_list = [] num_list = [] num = 0 finder(a) father_list = sorted(set(father_list),key=father_list.index)#列表去重复 nummax = max(num_list) #这里求了父节点有几代，因为计算方法是深度优先，取了最大值 writer.writerow([tab[\u0026#39;goAcc\u0026#39;][j],\u0026#34;,\u0026#34;,len(father_list),\u0026#34;,\u0026#34;,nummax,\u0026#34;,\u0026#34;,father_list]) csvfile.close() 计算Trichoderma reesei QM6a中每一个GO分别有多少个基因 方便起见，稍微改一改，另外建一个记录下级节点的字典。\ngocleanlist = [] #所有GO的非重复列表 with open(\u0026#39;2_clean.obo\u0026#39;,\u0026#39;r\u0026#39;) as f: while True: line = f.readline() strline = str(line) m = re.match(\u0026#39;id: GO\u0026#39;,line) if m is not None: gocleanlist.append(re.search(\u0026#39;GO:\\d{7}\u0026#39;,line).group()) if not line: break f.close() len(gocleanlist) #找所有儿子 def dictsonGO(): with open(\u0026#39;2_clean.obo\u0026#39;,\u0026#39;r\u0026#39;) as f: while True: line = f.readline() strline = str(line) m = re.match(\u0026#39;id: GO\u0026#39;,line) if m is not None: son_m = re.search(\u0026#39;GO:\\d{7}\u0026#39;,line).group() h = re.match(\u0026#39;is_a: GO|relationship: part_of\u0026#39;,strline) if h is not None: father = re.search(\u0026#39;GO:\\d{7}\u0026#39;,line).group() if father not in dict_son: dict_son[father] = [son_m] else: dict_son[father].append(son_m) if not line: break f.close() def finderson(a): son = dict_son.get(a) if son: son_list.extend(son) for i in son: if i in finished: #找完所有儿子并统计过的GO跳过，相当于剪枝了 pass else: finderson(i) dict_son ={} dictsonGO() #儿子计数 list_ =[] for j in range(0,len(tab[\u0026#39;goAcc\u0026#39;])-1): a = tab[\u0026#39;goAcc\u0026#39;][j] list_.append(a) result = pd.value_counts(list_) #将Trichoderma reesei QM6a里所有基因所在的GO计数，基因数量最高的GO有500+个基因 result_frame = result.to_frame() dict_result ={} #见字典好去索引，比如A是B的父级，B在该字典里，就把键值（基因数量）也给A加上 for i in range(0,len(result)-1): dict_result[result_frame[0].keys()[i]] = result_frame[0][i] 后面就是对所有GO进行计数，想了想，干脆一起把下一问也一起干了。\n即每一个GO中所属的基因显著上调和下调的个数有多少？\ndeg = pd.read_csv(\u0026#39;DEG.txt\u0026#39;,sep=\u0026#39;\\t\u0026#39;, encoding=\u0026#39;utf8\u0026#39;) Down = [] Up = [] #这个也不多，我循环套循环跑了，觉得浑身难受的就直接建字典。 for i in range(0,len(deg[\u0026#39;gene_id\u0026#39;])): for j in range(0,len(tab[\u0026#39;proteinId\u0026#39;])): if deg[\u0026#39;gene_id\u0026#39;][i] == tab[\u0026#39;proteinId\u0026#39;][j]: if deg[\u0026#39;significance\u0026#39;][i] == \u0026#39;DOWN\u0026#39;: Down.append(tab[\u0026#39;goAcc\u0026#39;][j]) if deg[\u0026#39;significance\u0026#39;][i] == \u0026#39;UP\u0026#39;: Up.append(tab[\u0026#39;goAcc\u0026#39;][j]) #列表建字典，主要后面搜索省时间 Down_dict = {} Up_dict = {} for i in Down: Down_dict[i] = 1 for i in Up: Up_dict[i] = 1 with open(\u0026#34;count_all.csv\u0026#34;,\u0026#34;w\u0026#34;,newline=\u0026#39;\u0026#39;) as csvfile: writer = csv.writer(csvfile, delimiter=\u0026#39; \u0026#39;) writer.writerow([\u0026#34;GO_id,\u0026#34;,\u0026#34;direct_Gene_number,\u0026#34;,\u0026#34;all_of_gene,\u0026#34;,\u0026#34;Down,\u0026#34;,\u0026#34;Up\u0026#34;]) #direct_Gene_number是直属于这个GO的基因，all_of_gene额外加上了下级GO的基因数量。 finished = [] # finderson 找完的记录在里面 finish_down={} # 顺便把down finish_up={} time =0 for a in gocleanlist: #可以加个输出计量运行到哪儿了 son_list = [] Down_num = 0 Up_num=0 n = dict_result.get(a,0) all_of_gene = n finderson(a) son_list = sorted(set(son_list),key=son_list.index) for z in son_list: all_of_gene = all_of_gene + dict_result.get(z,0) if z in Down_dict: if z in finish_down: Down_num = Down_num + finish_down[z] else: Down_num =Down_num+1 if z in Up_dict: if z in finish_up: Up_num = Up_num + finish_up[z] else: Up_num =Up_num+1 if all_of_gene == 0: #没有就不记了，省地方，不然得一堆0中找值 pass else: writer.writerow([a,\u0026#34;,\u0026#34;,n,\u0026#34;,\u0026#34;,all_of_gene,\u0026#34;,\u0026#34;,Down_num,\u0026#34;,\u0026#34;,Up_num]) finished.append(a) finish_down[a]=Down_num finish_up[a]=Up_num csvfile.close() #输出便是此番结果 GO_id, direct_Gene_number, all_of_gene, Down, Up GO:0000002 , 1 , 1 , 0 , 0 GO:0000003 , 0 , 1 , 0 , 0 GO:0000009 , 15 , 15 , 0 , 0 GO:0000015 , 2 , 2 , 0 , 0 GO:0000026 , 15 , 18 , 1 , 0 GO:0000030 , 19 , 104 , 5 , 0 GO:0000033 , 15 , 15 , 0 , 0 GO:0000036 , 3 , 3 , 0 , 0 GO:0000041 , 0 , 8 , 0 , 1 GO:0000049 , 1 , 1 , 0 , 0 GO:0000059 , 10 , 10 , 0 , 0 GO:0000062 , 2 , 2 , 0 , 0 GO:0000087 , 1 , 1 , 0 , 0 GO:0000096 , 0 , 11 , 0 , 0 GO:0000097 , 0 , 8 , 0 , 0 GO:0000103 , 2 , 3 , 0 , 0 GO:0000104 , 3 , 6 , 0 , 0 GO:0000105 , 7 , 7 , 0 , 0 GO:0000107 , 2 , 2 , 0 , 0 GO:0000121 , 1 , 1 , 0 , 0 GO:0000139 , 3 , 3 , 0 , 0 GO:0000140 , 8 , 8 , 0 , 0 GO:0000145 , 3 , 3 , 0 , 0 GO:0000148 , 2 , 2 , 0 , 0 GO:0000150 , 1 , 1 , 0 , 0 GO:0000151 , 56 , 58 , 0 , 0 ...... 哪些GO发生了显著的上调或者下调的富集？ 做GO的富集是算超几何分布，相比python，使用R更简单。\nlibrary(dplyr) setwd(\u0026#34;C:/Users/syddd/bioinfomation\u0026#34;) cou \u0026lt;- read.csv(\u0026#34;count_all.csv\u0026#34;) N = 13267+3043+2410 # 三个顶级GO的基因数量相加 n_down = 150 + 22 + 9 n_up = 222 + 32 + 16 df1\u0026lt;-data.frame(GO_id=c(), down_p_value=c(), up_p_value=c()) df2\u0026lt;-data.frame(GO_id=c(), down_p_value=c()) df3\u0026lt;-data.frame(GO_id=c(), up_p_value=c()) for (i in 1:nrow(cou)) { k_down \u0026lt;- cou[i,\u0026#34;Down\u0026#34;] k_up \u0026lt;- cou[i,\u0026#34;Up\u0026#34;] m \u0026lt;- cou[i,\u0026#34;all_of_gene\u0026#34;] p_down = 1-phyper(k_down-1,m, N-m, n_down) p_up = 1-phyper(k_up-1,m, N-m, n_up) list1 \u0026lt;- list(GO_id=cou[i,\u0026#34;GO_id\u0026#34;], down_p_value=p_down, up_p_value=p_up) df1 \u0026lt;- rbind(df1,as.data.frame(list1)) if (p_down \u0026lt;= 0.05){ #挑出来下调显著的 list2 \u0026lt;- list(GO_id=cou[i,\u0026#34;GO_id\u0026#34;], down_p_value=p_down) df2 \u0026lt;- rbind(df2,as.data.frame(list2)) } if (p_up \u0026lt;= 0.05){ #挑出来上调显著的 list3 \u0026lt;- list(GO_id=cou[i,\u0026#34;GO_id\u0026#34;], up_p_value=p_up) df3 \u0026lt;- rbind(df3,as.data.frame(list3)) } } ","date":"2021-06-11T20:11:25+08:00","image":"https://s2.loli.net/2021/12/03/obycVOJlgKFZpjA.png","permalink":"https://deathsprout.github.io/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%E5%AE%9E%E9%AA%8C-%E8%BD%AC%E5%BD%95%E7%BB%84%E5%AD%A6%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%92%8C%E5%AF%8C%E9%9B%86%E5%88%86%E6%9E%90/","title":"山东大学生物信息学实验 - 转录组学数据分析和富集分析"},{"content":"常见定年软件\n 最大似然法：  r8s Reltime   贝叶斯法：  PAML-MCMCTree（PAML4软件包内含） 其他（Timetree）    这里是mcmctree的手册\n事先声明，mcmctree使用氨基酸序列进行定年比使用核酸序列麻烦的多，而且在此之前，能找到帖子的都语焉不详，手册里的Tutorial 4是主要参考，可以先跑一遍示例文件，坑主要是体现在文件格式上。\n若使用氨基酸序列来进行分析，由于mcmctree不能选择较好的氨基酸替换模型进行分析，需要自己手动运行codeml进行分析后，在生成中间文件用于运行mcmctree。若非是因为数据原因必须用氨基酸序列进行定年，还是推荐使用核酸序列。\n前置 需求文件：(后面称其为那三个文件)\n 多序列比对文件 带有校准信息有根树 控制文件 mcmctree.ctl  前两个文件的格式很重要，参考examples里的abglobin.aa 和 abglobin.trees\n \n \n树文件一定保证有至少一个时间校正点，\u0026lsquo;B(1.7,1.9)\u0026lsquo;的意思是最外面的这个分支点的时间，经化石证据矫正约束在170Mya到190Mya1年之间。\n多序列对比文件第一行是： 序列数 序列长 （大写的）i 下面每行是一个序列名，和后面序列对应，序列名与序列之间的有空行，数字则是表明其下的序列们第一个位置是第几位。\n我将fasta文件转变为phylip后，如下图所示，序列名并没有单独成行，它的格式并不能直接用，会报错。我也没找到合适的现有格式能方便的转换过去。 我参考的其他博客都是将其转化为phylip就能正常使用，唯一的区别是他们使用核酸序列而我用的是氨基酸序列，这是我踩的坑。\n \n数字不要紧，我在第一行后面加了个I后，除了第一行以外都能识别出，下面是识别过程的输出。  \n所以重点是将\nG000238395 IVDLIDKVGL KDYQACCPFH NEKSPSFTVS QDKYHCFGCG ANGNAISFVM\nG000263075 IVDLIDKVGL KDYQACCPFH NEKSPSFTVS QDKYHCFGCG ANGNAISFVM\n这种形式变成下面那种形式\nG000238395\nG000263075\n（空行）\nIVDLIDKVGL KDYQACCPFH NEKSPSFTVS QDKYHCFGCG ANGNAISFVM\nIVDLIDKVGL KDYQACCPFH NEKSPSFTVS QDKYHCFGCG ANGNAISFVM\n数量大的话可以写个脚本，这里数量不多，用excel分割剪切出来，替换Tab键为空格再复制进去。\n流程 首先把那三个文件放在一个目录里，修改mcmctree文件，如下图红色部分所示。\n \nseqfile # 多序列比对文件\rtreefile # 带有校准信息有根树\rndata # 输入的多序列比对的数据个数，是密码子就是3；否则设置为1\rseqtype = 2 * 0: nucleotides; 1:codons; 2:AAs #数据类型(2为氨基酸)\rusedata = 1 * 0: no data; 1:seq like; 2:normal approximation; 3:out.BV (in.BV) # 设置是否利用多序列比对的数据：\\\r#0，表示不使用多序列比对数据，则不会进行likelihood计算，虽然能得到mcmc树且计算速度飞快，但是其分歧时间结果是有问题的；\\\r#1，表示使用多序列比对数据进行likelihood计算，正常进行MCMC，是一般使用的参数; \\\r#2，进行正常的approximation likelihood分析，此时不需要读取多序列比对数据，直接读取当前目录中的in.BV文件。该文件是使用usedata = 3参数生成的out.BV文件重命名而来的。\\\r#此外，由于程序BUG，当设置usedata = 2时，一定要在改行参数后加 *，否则程序报错 Error: file name empty.. \\\r#3，程序利用多序列比对数据调用baseml/codeml命令对数据进行分析，生成out.BV文件。由于mcmctree调用baseml/codeml进行计算的参数设置可能不太好（特别时对蛋白序列进行计算时），\\\r#推荐自己修改软件自动生成的baseml/codeml配置文件，然后再手动运行baseml/codeml命令，再整合其结果文件为out.BV文件。\r运行\nmcmctree mcmctree.ctl\r之后会生成一系列文件，删除out.BV和rst文件，将wag.dat拷贝进来， (wag.dat在paml dat 目录里，我是在..pamlX\\paml4.9j\\dat\\ 里面)\n打开文件tmp0001.ctl，全部替换为下列内容。\nseqfile = tmp0001.txt\rtreefile = tmp0001.trees\routfile = tmp0001.out\rnoisy = 3\rseqtype = 2\rmodel = 2 * 2: Empirical\raaRatefile = wag.dat\rfix_alpha = 0\ralpha = .5\rncatG = 4\rSmall_Diff = 0.1e-6\rgetSE = 2\rmethod = 1\r运行\ncodeml tmp0001.ctl\r这样就使用WAG+Gamma生成了适当的Hessian矩阵，接下来将rst2重命名为in.BV,现在可以更改mcmctree.ctl 的 usedata = 2\n回到上一目录，新建一个文件夹，将 那三个文件和新生成的in.BV拷贝进去\n接下来运行\nmcmctree mcmctree.ctl\r后续与mcmctree无关\n参考  MCMCTree tutorials mcmctree估算物种分歧时间 使用PAML进行分歧时间计算    Mya:百万年,mcmctree的单位是100个百万年\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"2021-05-25T06:52:23+08:00","permalink":"https://deathsprout.github.io/p/mcmctree-%E5%AE%9A%E5%B9%B4-%E4%BD%BF%E7%94%A8%E6%B0%A8%E5%9F%BA%E9%85%B8%E5%BA%8F%E5%88%97/","title":"mcmctree 定年 —— 使用氨基酸序列"},{"content":"chaotica 这个软件是我在一年前发现，那时候正满怀热情的学习Blender，在那之前还兴致勃勃的修完了许德民教授讲关于抽象艺术的尔雅网课。\n老师关于抽象艺术的讲解很大程度上改变了我对该类艺术作品的看法，说是醍醐灌顶也不为过（惭愧的是当时是第一次修习尔雅网课，忘记了最后的考试）。\n许德民教授很鼓励学生自己去创作抽象作品，也不要将其看的过高过于艰深遥远，他们课程的最后作业便是创做几幅抽象画了，我很是眼馋，然后用PS（上）和SAI 2（下）画了两幅抽象画。\n \n这张我很是满意，倒不是有什么寓意，有意思而且看着舒服罢了。\n \n（裁剪）本是像弄出影印版书封皮的质感的，灵感来源兴许是格林伍德元素化学。\n说了些无关的，介绍下本文的正主chaotica。 Chaotica是一款新一代分形艺术应用程序，新手用户可以享受编辑随机分形产生的惊人高清壁纸和动画。下面是他们的官方网站\n \nbilibili上有较为详细的入门教程，看完感兴趣可以去学学。\nhttps://www.bilibili.com/video/BV12f4y1X7JD/?spm_id_from=333.788.recommend_more_video.-1\n作品分享 我用chaotica制作了些图片，过程中偶尔出现惊喜，虽然没有chaotica里那些专业艺术家做的那么棒，但还是挺酷的，在这里分享给大家。\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n","date":"2021-05-24T16:09:47+08:00","image":"https://i.loli.net/2021/05/24/nQovGt7zCABKLXZ.png","permalink":"https://deathsprout.github.io/p/%E5%88%86%E5%BD%A2%E8%89%BA%E6%9C%AF-chaotica%E4%BD%9C%E5%93%81/","title":"分形艺术 Chaotica作品"},{"content":"关于笔记难免会选择性记忆，自己已然掌握的便不会记录，比如perl的正则表达式和python共通，便不记录在里面。（当然在这里更推荐去学python）\nsort #对数组排序（默认按照字符串排序） reverse #列表反向排列 sort{$a cmp $b}; #（由小到大） sort{$b cmp $a}; #（由大到小） sort{$a \u0026lt;=\u0026gt; $b}; #(按照数值进行排序) sort {$h{$a}\u0026lt;=\u0026gt;$h{$b}} key %h; #对哈希 比value排key # 用 . 可以将字符串连接 substr $总串,开始位置,长度,该内容替换被取出的内容;#用来取子串 index $字符串,\u0026#34;寻找的内容\u0026#34;,起始位置;#搜寻字符串内容，返回位置 my $str = \u0026#34;xal\u0026#34; x 5; #输出xalxalxalxalxal join \u0026#34;自定分隔符\u0026#34;,@arr; #连接数组中所有的字符串 split /自定分割符/,$str; #分割字符串 2021/04/28\n#! /usr/bin/perl use strict; use warnings; my @lines; #数组的每一项对应文件的每一行 my $tmp; #依次把每一行赋值给临时变量 open IN,\u0026#34;testout.txt\u0026#34; or die \u0026#34;fail to open testout.txt to read:$!\\n\u0026#34;; while ($tmp = \u0026lt;IN\u0026gt;) { chomp $tmp; #把字符串尾巴的换行符切掉 print\u0026#34;line:|\u0026#34;,$tmp,\u0026#34;|\\n\u0026#34;; } while (\u0026lt;IN\u0026gt;) { #因为上一个while以及把指针指到文件末尾了，此while将没有输出 print\u0026#34;line:|\u0026#34;,$_,\u0026#34;|\\n\u0026#34;;#临时变量 } close IN; open IN,\u0026#34;testout.txt\u0026#34; or die \u0026#34;fail to open testout.txt to read:$!\\n\u0026#34;; while (\u0026lt;IN\u0026gt;) { chomp $_; print\u0026#34;line:|\u0026#34;,$_,\u0026#34;|\\n\u0026#34;;#临时变量 } close IN; open OUT,\u0026#34;\u0026gt;testout.txt\u0026#34; or die \u0026#34;fail to open testout.txt to write:$!\u0026#34;;# $!是存放最近的出错信息的变量 print OUT \u0026#34;this is the 1 line.\\n\u0026#34;; print OUT \u0026#34;this is the 2 line.\\n\u0026#34;; close OUT; my @arr = (\u0026#34;list\u0026#34;); $arr[$#arr+1] = \u0026#34;new\u0026#34;;#尾添 push @arr,\u0026#34;new_push\u0026#34;;#尾添 print \u0026#34;@arr\\n\u0026#34;; pop @arr;#尾删 print \u0026#34;@arr\\n\u0026#34;; shift @arr;#头删 print \u0026#34;@arr\\n\u0026#34;; unshift @arr,\u0026#34;interposition\u0026#34;;#头插入 print \u0026#34;@arr\\n\u0026#34;; my $return; $return = shift @arr;# 将@arr的头删除并将其作为返回值赋值给$return print\u0026#34;$return\\n @arr\\n\u0026#34;; # $return = shift; #在运行perl文件时输入数据作为参数 # such as perl input_output.pm 1 2 3 $return = shift @ARGV;# 通过添加@ 使其能添加多个参数 if (not $return){ die \u0026#34;usage:perl arg.plx\u0026lt;paramter\u0026gt;\\n\u0026#34;; }#没给参数，杀死并报错 print\u0026#34;$return\\n @arr\\n\u0026#34;; # 使得脚本可以接受文件名作为参数，打开该文件并逐行输出 my $filename; $filename = shift; if (not $filename){die \u0026#34;usage:perl arg.plx\u0026lt;paramter\u0026gt;\\n\u0026#34;;} open IN,$filename or die \u0026#34;fail to open $filename to write:$!\u0026#34;; while (\u0026lt;IN\u0026gt;) { chomp $_; print $_,\u0026#34;\\n\u0026#34;; } close IN; 读取fasta文件，读取并输出序列信息和序列本身。\n#! /usr/bin/perl use strict; use warnings; my $filename; my @seq = ();my @name = (); my $cnt = 0; my $lnbuf ;my $ch ; my $tmpname;my$tmpseq; $filename = shift; if (not $filename){die \u0026#34;usage:perl arg.plx\u0026lt;paramter\u0026gt;\\n\u0026#34;;} open IN,$filename or die \u0026#34;fail to open $filename to write:$!\u0026#34;; while (\u0026lt;IN\u0026gt;) { chomp; $ch = substr $_,0,1; if ($ch eq \u0026#34;\u0026gt;\u0026#34;){ if ($cnt == 0){ $tmpname = $_; # push @name,$_; $cnt ++; } else{ push @name,$tmpname; push @seq,$tmpseq; $tmpseq = \u0026#34;\u0026#34;; $cnt ++; $tmpname=$_;} } else { $tmpseq .= $_; # 连接缩写 .= # @seq[$cnt-1] .= $_; } } close IN; if($tmpname ne \u0026#34;\u0026#34;){ push @name,$tmpname; push @seq,$tmpseq; } foreach my $i(0 .. $#name){ print $name[$i],\u0026#34;\\n\u0026#34;,$seq[$i],\u0026#34;\\n\u0026#34;; } Perl的正则表达式的三种形式，分别是匹配，替换和转化:\n  匹配：m//（还可以简写为//，略去m）\n  替换：s///\n  转化：tr///\n  这三种形式一般都和 =~ 或 !~ 搭配使用， =~ 表示相匹配，!~ 表示不匹配。\nperl处理完后会给匹配到的值存在三个特殊变量名:\n  $`: 匹配部分的前一部分字符串\n  $\u0026amp;: 匹配的字符串\n  $': 还没有匹配的剩余字符串\n  my $str = \u0026#34;Hello the perl world\u0026#34;; $str =~/Hello/; #搜索 搜到返回为真 $str =~/Hello/ ? print \u0026#34;found Hello\\n\u0026#34; : print \u0026#34;can\u0026#39;t found\\n\u0026#34; ; my $str2 = \u0026#34;\u0026gt;seq1 \u0026gt;seq2 \u0026gt;seq3\u0026#34;; $str2 =~/^\u0026gt;/ ? print \u0026#34;found \u0026gt; at begin\\n\u0026#34; : last ; # ^ 锚定到句首 # $ 锚定到句末， bp$ 搜索句末的bp my $str3 = \u0026#34;total length 10000000 bp, N50 20000 bp\u0026#34;; $str3 =~/total length (\\d{1,}) bp/ ? my $tot_len = $1 : last ; print $tot_len,\u0026#34;\\n\u0026#34;; # () 里面的值会被抓取出来，存在临时变量里 第一个放在 $1,第二个放在 $2 ... # \\d 匹配任何Unicode十进制数（就是在Unicode字符目录[Nd]里的字符） ## 这包括了 [0-9] ，和很多其他的数字字符。如果设置了 ASCII 标志，就只匹配 [0-9]  # {1,} 表示匹配次数，1次以上 my $str4 = \u0026#34;joadfffff the\u0026#34;; $str4 =~/(f*)\\s*(the)/ ? my $jiaf = $1.$2 : last ; print $jiaf,\u0026#34;\\n\u0026#34;; # \\s 匹配任何Unicode空白字符（包括 [\\t\\n\\r\\f\\v]) # *? \u0026#39;*\u0026#39;, \u0026#39;+\u0026#39;，和 \u0026#39;?\u0026#39; 修饰符都是贪婪的,在后面加？，表示非贪婪匹配 my $str5 = \u0026#34;01234569acespvnmgogjbtfgg\u0026#34;; $str5 =~/([0-9a-h]+)/ ? print $1,\u0026#34;\\n\u0026#34; : last ; $str5 =~/([^0-9a-h]+)/ ? print $1,\u0026#34;\\n\u0026#34; : last ; # [^ ]表示反选字符集合 my $string = \u0026#39;4runoooob2,57\u0026#39;; $string =~ tr/a-z/A-Z/s ; print \u0026#34;$string\\n\u0026#34;; # s把多个相同的输出字符缩成一个 $string =~ tr/N/ /c ; print \u0026#34;$string\\n\u0026#34;; # 把所有非N 替换为空格 $string =~ tr/\\t //d ; print \u0026#34;$string\\n\u0026#34;; # 删除tab和空格 $string =~ tr/0-9/ /cs ; print \u0026#34;$string\\n\u0026#34;; # 把数字间的其它字符替换为一个空格。 引用：用来弄高维数据结构（实际是指针）\nmy $addr = \\$str ; print $addr,\u0026#34;\\t\u0026#34;,$$addr,\u0026#34;\\n\u0026#34;; # \\$str 取地址，| ${ $addr } 解引用，也可缩写为 $$addr #输出： SCALAR(0x7fffc0a84060) Hello the perl world my @arr = (1 .. 7); my $addr2 = \\@arr ; print $addr2,\u0026#34;\\t\u0026#34;,\u0026#34;@{ $addr2 }\u0026#34;,\u0026#34;\\t\u0026#34;,${ $addr2 }[0],\u0026#34;\\n\u0026#34;; # ARRAY(0x7ffff5b228c8) 1 2 3 4 5 6 7 1 my %h = (a=\u0026gt;1,b=\u0026gt;2,c=\u0026gt;3); my $addr3 = \\%h; print $addr3,\u0026#34;\\t\u0026#34;,keys %{$addr3},\u0026#34;\\n\u0026#34;; print ref($addr),\u0026#34;\\t\u0026#34;,ref($addr2),\u0026#34;\\t\u0026#34;,ref($addr3); # ref() 返回引用的类型 # HASH(0x7fffdae689e8) cab my @arrac1 = split / +/,\u0026#34;4 -1 -4 0 1\u0026#34;; my @arrac2 = split / +/,\u0026#34;2 -3 -4 0 1\u0026#34;; my @arrac3 = split / +/,\u0026#34;1 -5 -4 0 1\u0026#34;; my @arr_of_arr ; $arr_of_arr[0]= \\@arrac1;$arr_of_arr[1]= \\@arrac2;$arr_of_arr[2]= \\@arrac3; foreach my $i (0 .. $#arr_of_arr){ print $i,\u0026#34;\\t\u0026#34;,$arr_of_arr[$i],\u0026#34;\\t\u0026#34;,\u0026#34;@{$arr_of_arr[$i]}\u0026#34;,\u0026#34;\\n\u0026#34;; }# 演示，非实际用法，麻烦的写法 @{ $arr_of_arr[0] } = split / +/,\u0026#34;1 2 3 4 5\u0026#34;; # 省事操作 foreach my $i (0 .. $#arr_of_arr){ print $i,\u0026#34;:\u0026#34;; foreach my $j (0 .. $#{$arr_of_arr[$i]}){ print \u0026#34; \u0026#34;,$j,\u0026#34;(\u0026#34;,$arr_of_arr[$i][$j],\u0026#34;)\u0026#34;; } print \u0026#34;\\n\u0026#34;; } # can $arr_of_arr[$i] -\u0026gt; [$j] my $arrlist = [ [9,8,7,6,5], [1,2,3,4,5], [5,5,5,5,5], ]; #匿名数组 print @$arrlist,\u0026#34;\\t\u0026#34;,\u0026#34;@{@$arrlist[0]}\u0026#34;;   上图是部分输出结果\n# 计算拼接成contig的文件 N50的perl程序 use strict; use warnings; my @name;my @seq;my $seq; my @long ; my $alllong = \u0026#34;\u0026#34;; my $contig = 0 ; my $longest; my $shortest; my $GC=0 ; my $N=0; my $N50; open IN,\u0026#34;data/a9_k63.contig.long1500\u0026#34; or die \u0026#34;$!\u0026#34;; # a9_k63.contig.long1500 while (\u0026lt;IN\u0026gt;) { chomp; my $head = substr $_,0,1; if ($head eq \u0026#34;\u0026gt;\u0026#34;){ if ($contig == 0){ $contig ++; push @name,$_; } else{ $contig ++; push @name,$_; $alllong .= $seq ; push @seq,$seq; push @long,length($seq); $seq = \u0026#34;\u0026#34;; } } else{ $seq .= $_; } } push @seq,$seq; $alllong .= $seq ; push @long,length($seq); close IN; my @arrlong = sort{$a\u0026lt;=\u0026gt;$b} @long; $longest = $arrlong[0];$shortest = $arrlong[$#long]; my $G = $alllong =~ tr/G/G/; my $C = $alllong =~ tr/C/C/; $N = $alllong =~ tr/N/N/; $GC = ($G+$C)/length($alllong); my $sum =0;my $lo2 = length($alllong)/2; foreach my $i (0 .. $#arrlong){ $sum = $sum+$arrlong[$i]; if ($sum \u0026gt; $lo2){ $N50 = $arrlong[$i]; last; } } print \u0026#34;@long\\n\u0026#34;; print \u0026#34;大小：\u0026#34;,length($alllong),\u0026#34;\\tcontig数:\u0026#34;,$contig,\u0026#34;\\n\u0026#34;; print \u0026#34;最短contig：\u0026#34;,$longest,\u0026#34;\\t最长contig:\u0026#34;,$shortest,\u0026#34;\\n\u0026#34;; print \u0026#34;GC含量:\u0026#34;,$GC,\u0026#34;\\t\\tN数量:\u0026#34;,$N,\u0026#34;\\n\u0026#34;; print \u0026#34;N50:\u0026#34;,$N50,\u0026#34;\\n\u0026#34;; ","date":"2021-05-22T19:35:28+08:00","image":"https://i.loli.net/2021/12/03/b5iCUtcZKnwF1BJ.jpg","permalink":"https://deathsprout.github.io/p/perl%E5%AE%9E%E9%AA%8C%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/","title":"Perl实验课程笔记"},{"content":"系统：Ubuntu 20.0.4\n轮子：ena-fast-download\n环境需求：\n An Aspera connect client (see https://www.ibm.com/aspera/connect/ or https://www.biostars.org/p/325010/) curl(Linux自带，但版本不要太旧) Python 3  Aspera 配置 在 https://www.ibm.com/aspera/connect/下载合适的IBM Aspera Connect。解压结束在shell运行那个.sh文件，安装IBM Aspera Connect。\n配置$PATH,手动配置是在/home/user（用户名,我的是sprout）位置的.bashrc文件最后添加\nexport PATH=/home/sprout/.aspera/connect/bin:$PATH asperaweb_id_dsa.putty 和asperaweb_id_dsa.openssh是密匙文件，要在最后ascp使用时用到，在后面加 -i 密匙文件的位置，正常使用ascp是如果putty不能用就用openssh，这个脚本ena-fast-download用的密匙是openssh，所以环境变量也要加上。\nexport PATH=/home/sprout/.aspera/connect/etc/asperaweb_id_dsa.openssh:$PATH 使用 Linux下是将ena-fast-download这里面的python脚本放到想下数据的文件夹中，执行下面的命令。\n./ena-fast-download.py ERR1701760 实际上这个脚本是用ERR1701760得到完整的路径去使用ascp，比如上面这个最终会生成(其中一个，双端测序数据，还一个2)\nascp-T-1 300m -P33001-i $HOME/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/ERR1701/000/ERR1701760/ ERR1701760 1.fastq.gz . 问题 我一开始使用下着下着会中断，不清楚是什么问题，可能是Ubuntu长时间不动熄屏导致，也可能是流程里有什么东西变了导致现在用不了。 单纯的运行ascp脚本报错如下：\nSession Stop (Error:Session data transfer timeout(server),Session data transfer timeout) 后来发现是实验室网络问题，在宿舍运行速度可以达到60MB/s。\n","date":"2021-05-22T13:23:33+08:00","permalink":"https://deathsprout.github.io/p/%E4%BB%8Eena%E9%AB%98%E9%80%9F%E4%B8%8B%E8%BD%BDfastq%E6%96%87%E4%BB%B6/","title":"从ENA高速下载FASTQ文件"},{"content":"同样发表在知乎上\n为什么写这篇文章？ 这是我在为完成《蛋白质工程》课程展示任务过程中写的，选题自然是cas蛋白，我发现在网络和各类博客上反而很难找到cas9蛋白的具体内容，它被介绍CRISPR-Cas的文章淹没了，在读了十几篇文献后，用latex写了笔记。鉴于关于CRISPR-Cas的内容随处可见，完全可以去看产品手册或者其他什么，这篇文章中删去了与Cas蛋白无直接关系的大量内容。\nCas蛋白简介 Cas是CRISPR相关蛋白的简称， 簇状规则间隔的短回文重复序列（CRISPR）和相关蛋白（Cas）构成CRISPR–Cas系统1，是许多细菌和大多数古细菌中存在的抗噬菌体免疫系统。 近年来，CRISPR–Cas系统已发展成为可靠而强大的基因组编辑工具。\n CRISPR系统的三大类和十个子类的关系  上图1表示有三大类的CRISPR系统（I、II、III），而又可以细分为十个子类，可以看出在CRISPR系统中含有很多种类的Cas蛋白，Cas1到Cas10以及如Cas8a1、Cas12a、Cas13a等比较稀少的类型。2\n第2类CRISPR-Cas系统的特征是由单一、大、多结构域的蛋白质组成复合体发挥作用，一些2类效应蛋白，如Cas9,已经成功地用于基因组工程。3\n 基于不同的效应蛋白家族,CRISPR II类系统又可细分为3类、9亚型 \n 不同种类的Cas蛋白数量分布  上图3则是CasPDB数据库统计的在不同种类的菌体中所含的Cas蛋白数量分布，在统计的所有 Cas 蛋白中，Cas1占20.11% ，cas2占17.46% ，Cas3、 Cas5和 Cas6占10% 以上。其余的较少，最著名的Cas9蛋白则在所有Cas蛋白中占2.59%，本文也将重点讨论Cas9蛋白。\nCas9蛋白结构 Cas9蛋白立体结构   SpCas9 -ABE8e 结构, 是使用PyMOL将从PDB数据库中获取的Cas9结构简单的可视化后的图像。4\n  3.2 A分辨率下的SpCas9-ABE8e复合物的冷冻电镜建模结构, SpCas9-ABE8e 冷冻电镜结构：SpCas9灰色显示； sgRNA紫色； 靶链DNA蓝绿色； 非靶链DNA蓝色； TadA-8e二聚体红色和粉红色显示。 Cas9 N端与TadA-8e C端之间的直线距离显示为橙色虚线。\nCas9蛋白结构域   Cas9蛋白结构域分为REC结构域、Ruvc结构域、HNH结构域、PI结构域,图6还特地将富含精氨酸的alpha螺旋用紫色标示出来。其中REC结构域可以分为REC1-A、REC2、REC1-B、REC3，作用是连接其他结构域。Ruvc结构域分为三个结构域，彼此之间并不靠近。\n  a是靶链DNA和sgRNA结合示意图。b是sgRNA-靶链DNA连接的正交视图。c是Cas9–sgRNA–DNA复合体的前视图和后视图。 在上图7中，RNA均为橙色，靶链DNA为浅蓝色，非靶链DNA为黑色。 非靶链中的5'-NGG-3\u0026rsquo;PAM三核苷酸以黄色突出显示。图中蛋白质上不同的颜色显示了不同的结构域。5\nCas9蛋白功能及作用机制 Cas9蛋白在CRISPR-Cas系统中的功能  上图8显示的是Type II 系统的作用过程。3 首先是依靠Cas1、2复合体将原间隔序列的片段制作成间隔序列整合进CRISPR序列中，形成对外源DNA的记忆，然后在接下来的免疫中，间隔序列就能快速产生原间隔序列片段，这些序列片段与Cas9一起靶向地切除在细菌内的外源DNA，PAM在这里参与防止自身免疫疾病的机制，保障不会切除自身DNA。\nCas9蛋白功能的机制 结合DNA并识别目标序列   上图是SpCas9寻找目标位点的步骤示意图，apo-Cas9 与DNA相互作用靠的是随机碰撞，结合后很快分开。在apo-Cas9 与sgRNA 结合后形成Cas9 RNP，使SpCas9发生形态和功能改变，允许目标通过沿双链DNA 的进行单向搜索约27个bp（非靶链DNA-3\u0026rsquo;至5'），除非遇到PAM，否则就快速的与DNA分离。与 PAM 的稳定相互作用则促使互补底物上稳定R环的形成，从而激活 Cas9以进行 DNA 切割。6\n就这样借助sgRNA的部分序列与靶DNA位点进行碱基配对，能够引导Cas9结合到这个靶位点上并进行切割。II型和V-B型系统需要tracrRNA才能正常发挥功能，而V-A型系统则需要单独使用crRNA 。在实际应用时，人们可以将tracrRNA和crRNA作为两种向导RNA（gRNA）或者融合在一起形成单向导RNA（single guide RNA, sgRNA），后一种方式已经广泛用于Cas9引导酶Cas9结合到靶DNA序列上并进行切割，Cas9与sgRNA一起被称作Cas9-sgRNA系统。\n  a.Cas9中PAM结合区的放大视图。b.Cas9与PAM相互作用示意图,红圈表示桥接水分子。c.主要凹槽的详细视图。 与GG-PAM的特异性氢键相互作用用虚线表示。 非靶链中的 dG2 * 和 dG3 * 的鸟嘌呤核酸碱基在主槽中分别被 arg1333和 arg1335的碱基特异性氢键相互作用读出， 这些氢键来自 Cas9 C末端的 $\\beta$发夹，而与PAM互补的靶链核苷酸不被主沟槽相互作用识别。与PAM相互作用的精氨酸残基在Cas9序列中是保守的。7\n \n残基Glu 1108和ser 1109与靶DNA链中的+1磷酸二酯基团相互作用，直接导致PAM上游的局部链分离，将靶链转为与sgRNA结合（1-2bp），靶链DNA与sgRNA之间的碱基配对促进了引导-靶标异质双链的进一步逐步置换和传播。7\n \n切割 Cas9的基因组编辑能力只有在被称作前间隔序列邻近基序(protospacer adjacent motif, PAM)的短片段DNA序列的存在下才成为可能。只有DNA靶位点附近存在PAM时，Cas9才能进行准确切割。PAM的存在也是激活酶Cas9所必需的，PAM 序列的特征为 NGG（其中 N 为任意核苷酸）。8\n核酸酶Cas9含有两个具有切割活性的结构域：HNH结构域和RuvC结构域，其中 HNH结构域切割与crRNA互补的DNA链，而RuvC结构域切割非互补链(即HNH和 RuvC结构域协同切割TS和NTS)。RuvC结构域可分为三个亚结构域：RuvC I、RuvC II和RuvC III，RuvC I接近于Cas9的氨基端，RuvC II和RuvC III位于HNH结构域的两侧（见图5）。9\n当Cas9与靶基因位点结合时发生了构象变化，核酸酶功能区对靶标DNA的反向链进行定位切割。Cas9介导的DNA切割最后结果是目标DNA（PAM序列上游约 3～4个核苷酸）的双链断裂（double strand break，DSB）。   上图是切割位置示意图，实验中观察到的切割位点由黑色（HNH）和蓝色（RuvC）三角形标记（蓝色填充:首选位点，无填充:典型位点(较不利)，青色填充:不利位点）。10\n传统的Cas9蛋白包含RuvC和HNH两个催化结构， RuvC和HNH可分别剪切DNA的两条链形成双链断裂，如gRNA与非目标区域结合就有造成不必要插入突变的可能。\n其他 存在的问题 基因组极为复杂，gRNA可能与非靶向序列局部匹配，这种局部匹配也会激活Cas 9内切酶活性，从而产生脱靶效应。11\n此外，Cas9不仅识别标准PAM，也可识别非标准PAM，这也可能会引起一定程度的脱靶。脱靶可能影响正常基因的功能表达，甚至激活致癌因子、抑制抑癌基因，造成安全隐患，这极大的阻碍了该技术在临床的进一步应用。目前主要通过优化gRNA 设计，改造Cas9蛋白，使用RNP递送方式等策略来提高CRISPR/Cas9系统特异性以降低脱靶现象。\nCas9蛋白突变体 博德研究所的研究人员对Cas9进行突变使RuvC和HNH两个催化结构域中的一个缺失核酸酶活性形成Cas9n，Cas9n与DNA双链作用时仅产生单链切口。应用CRISPR/Cas9n系统进行基因编辑需要使用两个相邻且相反链上的gRNA序列。虽然所用的每条gRNA的脱靶结合位点可能出现在全基因组范围内，但是Cas9n仅催化每个位置的单链断裂 （Single-strand break，SSB）。SSB优先通过HDR进行修复，而不是NHEJ，这可降低不必要的插入缺失突变的发生。应用CRISPR/Cas9n系统进行基因编辑能将脱靶活性降低50-1000倍。12\n  本文不介绍与Cas蛋白无关的CRISPR–Cas技术细节，有兴趣的详见 CRISPR Guide\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Makarova K S , Haft D H , Barrangou R , et al. Evolution and classification of the CRISPR–Cas systems[J]. Nature Reviews Microbiology.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Makarova K S , Zhang F , Koonin E V . SnapShot: Class 2 CRISPR-Cas Systems[J]. Cell, 2017, 168(1-2):328-328.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n DOI: 10.2210/pdb6VPC/pdb\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Anders, Carolin, Niewoehner, et al. Structural basis of PAM-dependent target DNA recognition by the Cas9 endonuclease.[J]. Nature, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Lapinaite A , Knott G J , Palumbo C M , et al. DNA capture by a CRISPR-Cas9–guided adenine base editor[J]. ence, 2020, 369(6503):566-571.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Anders, Carolin, Niewoehner, et al. Structural basis of PAM-dependent target DNA recognition by the Cas9 endonuclease.[J]. Nature, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Nierzwicki U , Arantes P R , Saha A , et al. Establishing the allosteric mechanism in CRISPR‐Cas9[J]. Wiley Interdiplinary Reviews: Computational Molecular ence.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Huai C , Li G , Yao R , et al. Structural insights into DNA cleavage activation of CRISPR-Cas9 system[J]. Nature Communications, 2017, 8(1):1375.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Stephenson A , Raper A T , Suo Z . Bidirectional Degradation of DNA Cleavage Products Catalyzed by CRISPR/Cas9[J]. Journal of the American Chemical Society, 2018, 140(10):3743.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Kuscu, C., Arslan, S., Singh, R., Thorpe, J. \u0026amp; Adli, M. Genome-wide analysis reveals characteristics of off-target sites bound by the Cas9 endonuclease. Nat Biotechnol 32, 677-683, doi:10.1038/nbt.2916 (2014)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Slaymaker, l. M. et al. Rationally engineered Cas9 nucleases with improved specificity. Science 351, 84-88, doi:10.1126/science.aad5227 (2016)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"2021-05-15T22:38:53+08:00","image":"https://pica.zhimg.com/v2-ffc7d038737eaeb8f680cf26b6b45df1_1440w.jpg?source=172ae18b","permalink":"https://deathsprout.github.io/p/cas-protein/","title":"Cas Protein"},{"content":"人总有表达自我的需求，从高中到现在大三，这种需求愈发强，宣泄通路倒是逐渐弱。现在不同的领域有各自的论坛、各自的账号，也无非是一个侧写，于我，于他/她，虽然这样说，这个个人博客也只不过是一些想让别人看到的东西罢了。\n我喜欢的东西很多，除本专业外，美术、音乐、哲学、物理、科幻、共产主义等等都很喜欢，自然会的挺杂。就拿美术举例，板绘参加过明日方舟的同人企划，给学校弄过些平面设计，会用blender建模，但我自己现在什么水平我自己清楚，半吊子一个不上不下，比如本来3D建模目的只是渲染出来好看就行，一到给做游戏的同学建模型时就各种破面各种问题，自己谱写的音乐少有灵动多半是狗屎。我懂分散精力在各个领域会导致我学的更浅，我体会得到自己的业余，自己毕竟不能只为了提升3D建模理解去硬啃计算机视觉的内容（或许看不懂还得有前置内容），倒也没有退缩，头脑里的莽劲已经让我将其视为足以坚持一生的爱好。\n在无意中接触到Latex，实在是优美，我在大三上学期决定用latex完全替代word的需求，包括实验报告与笔记，到现在也写八九十个文档了。特别是90页、2500行左右Latex代码的实习报告，花费了我极大的精力，从苦痛慢慢变成享受，精美的完成品PDF给了我很大的成就感，但除我之外估计只有课程老师能去抽出时间草草的浏览一遍，是躺在硬盘里可有可无的东西。\n其他笔记也不外如此，直到一次师兄让我去寻找并下载一组海洋宏基因组数据，我测试完整理了工具和流程，将跑出的PDF发给了师兄，师兄一脸新奇的说：“你写的好像博客哎”。这句话直接促使我产生了搭建个人博客的想法，详细解释一下，生物信息学专业涉及编程或技术细节上的内容，往往需要去阅读相关的手册，有幸不乏喜好惠泽后人的前辈，网上散着一些他们整理好的教程、帖子，实际做之前浏览阅读一些，常常省时省力。看多了，就能发现很多的精美的文章，出现在各式各样的个人博客里，也是因此我才了解了个人博客这一形式。\n我并非没有考虑过使用现有的平台写东西，知乎、CSDN、简书等都挺不错，一个高中朋友弄了个微信公众号也不错，自带平台的搜索权重岂不美哉。但多半是我想折腾，一开始打算使用我的树莓派4B作为服务器挂载这个网站（前段时间还用它挖了一天门罗币，产出和大肠杆菌鞭毛一样，无聊的紧），后来发现我并不需要一个动态的网站，仅仅挂载静态网页就可，鉴于在宿舍总不能一直开着树莓派，转向了借由github的服务挂载网页。便随意买了个网址，解析地址配置好，之后便发现使用github挂载的网页不科学上网不能稳定访问，最后就选择使用gitee了。\n现在这个页面排版是基于HugoTex theme的，原因自然是它的LaTex风格。虽然对它的css和toml等配置文件进行大量改动，增加了一些功能，但它的功能仍是贫弱。现在文章尚少还能接受，推测随着文章数目的增多，我会更改theme，又免不了一阵折腾。\n至于体会，一个长久居住在不同的集体宿舍的人，自己用别人提供的简单方法盖了个简陋的房子。\n","date":"2021-05-12T22:38:53+08:00","permalink":"https://deathsprout.github.io/p/%E4%B8%80%E4%BA%9B%E4%B8%8D%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%8B%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BB%BA%E8%BF%99%E4%B8%AA%E5%8D%9A%E5%AE%A2/","title":"一些不重要的事——为什么建这个博客"}]